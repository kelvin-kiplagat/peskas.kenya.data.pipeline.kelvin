[{"path":[]},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/CODE_OF_CONDUCT.html","id":"our-pledge","dir":"","previous_headings":"","what":"Our Pledge","title":"Contributor Covenant Code of Conduct","text":"members, contributors, leaders pledge make participation community harassment-free experience everyone, regardless age, body size, visible invisible disability, ethnicity, sex characteristics, gender identity expression, level experience, education, socio-economic status, nationality, personal appearance, race, caste, color, religion, sexual identity orientation. pledge act interact ways contribute open, welcoming, diverse, inclusive, healthy community.","code":""},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/CODE_OF_CONDUCT.html","id":"our-standards","dir":"","previous_headings":"","what":"Our Standards","title":"Contributor Covenant Code of Conduct","text":"Examples behavior contributes positive environment community include: Demonstrating empathy kindness toward people respectful differing opinions, viewpoints, experiences Giving gracefully accepting constructive feedback Accepting responsibility apologizing affected mistakes, learning experience Focusing best just us individuals, overall community Examples unacceptable behavior include: use sexualized language imagery, sexual attention advances kind Trolling, insulting derogatory comments, personal political attacks Public private harassment Publishing others’ private information, physical email address, without explicit permission conduct reasonably considered inappropriate professional setting","code":""},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/CODE_OF_CONDUCT.html","id":"enforcement-responsibilities","dir":"","previous_headings":"","what":"Enforcement Responsibilities","title":"Contributor Covenant Code of Conduct","text":"Community leaders responsible clarifying enforcing standards acceptable behavior take appropriate fair corrective action response behavior deem inappropriate, threatening, offensive, harmful. Community leaders right responsibility remove, edit, reject comments, commits, code, wiki edits, issues, contributions aligned Code Conduct, communicate reasons moderation decisions appropriate.","code":""},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/CODE_OF_CONDUCT.html","id":"scope","dir":"","previous_headings":"","what":"Scope","title":"Contributor Covenant Code of Conduct","text":"Code Conduct applies within community spaces, also applies individual officially representing community public spaces. Examples representing community include using official e-mail address, posting via official social media account, acting appointed representative online offline event.","code":""},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/CODE_OF_CONDUCT.html","id":"enforcement","dir":"","previous_headings":"","what":"Enforcement","title":"Contributor Covenant Code of Conduct","text":"Instances abusive, harassing, otherwise unacceptable behavior may reported community leaders responsible enforcement l.longobardi@cgiar.org. complaints reviewed investigated promptly fairly. community leaders obligated respect privacy security reporter incident.","code":""},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/CODE_OF_CONDUCT.html","id":"enforcement-guidelines","dir":"","previous_headings":"","what":"Enforcement Guidelines","title":"Contributor Covenant Code of Conduct","text":"Community leaders follow Community Impact Guidelines determining consequences action deem violation Code Conduct:","code":""},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/CODE_OF_CONDUCT.html","id":"id_1-correction","dir":"","previous_headings":"Enforcement Guidelines","what":"1. Correction","title":"Contributor Covenant Code of Conduct","text":"Community Impact: Use inappropriate language behavior deemed unprofessional unwelcome community. Consequence: private, written warning community leaders, providing clarity around nature violation explanation behavior inappropriate. public apology may requested.","code":""},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/CODE_OF_CONDUCT.html","id":"id_2-warning","dir":"","previous_headings":"Enforcement Guidelines","what":"2. Warning","title":"Contributor Covenant Code of Conduct","text":"Community Impact: violation single incident series actions. Consequence: warning consequences continued behavior. interaction people involved, including unsolicited interaction enforcing Code Conduct, specified period time. includes avoiding interactions community spaces well external channels like social media. Violating terms may lead temporary permanent ban.","code":""},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/CODE_OF_CONDUCT.html","id":"id_3-temporary-ban","dir":"","previous_headings":"Enforcement Guidelines","what":"3. Temporary Ban","title":"Contributor Covenant Code of Conduct","text":"Community Impact: serious violation community standards, including sustained inappropriate behavior. Consequence: temporary ban sort interaction public communication community specified period time. public private interaction people involved, including unsolicited interaction enforcing Code Conduct, allowed period. Violating terms may lead permanent ban.","code":""},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/CODE_OF_CONDUCT.html","id":"id_4-permanent-ban","dir":"","previous_headings":"Enforcement Guidelines","what":"4. Permanent Ban","title":"Contributor Covenant Code of Conduct","text":"Community Impact: Demonstrating pattern violation community standards, including sustained inappropriate behavior, harassment individual, aggression toward disparagement classes individuals. Consequence: permanent ban sort public interaction within community.","code":""},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/CODE_OF_CONDUCT.html","id":"attribution","dir":"","previous_headings":"","what":"Attribution","title":"Contributor Covenant Code of Conduct","text":"Code Conduct adapted Contributor Covenant, version 2.1, available https://www.contributor-covenant.org/version/2/1/code_of_conduct.html. Community Impact Guidelines inspired [Mozilla’s code conduct enforcement ladder][https://github.com/mozilla/inclusion]. answers common questions code conduct, see FAQ https://www.contributor-covenant.org/faq. Translations available https://www.contributor-covenant.org/translations.","code":""},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/CONTRIBUTING.html","id":null,"dir":"","previous_headings":"","what":"Contributing to peskas.kenya.data.pipeline","title":"Contributing to peskas.kenya.data.pipeline","text":"outlines propose change peskas.kenya.data.pipeline. detailed discussion contributing tidyverse packages, please see development contributing guide code review principles.","code":""},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/CONTRIBUTING.html","id":"fixing-typos","dir":"","previous_headings":"","what":"Fixing typos","title":"Contributing to peskas.kenya.data.pipeline","text":"can fix typos, spelling mistakes, grammatical errors documentation directly using GitHub web interface, long changes made source file. generally means ’ll need edit roxygen2 comments .R, .Rd file. can find .R file generates .Rd reading comment first line.","code":""},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/CONTRIBUTING.html","id":"bigger-changes","dir":"","previous_headings":"","what":"Bigger changes","title":"Contributing to peskas.kenya.data.pipeline","text":"want make bigger change, ’s good idea first file issue make sure someone team agrees ’s needed. ’ve found bug, please file issue illustrates bug minimal reprex (also help write unit test, needed). See guide create great issue advice.","code":""},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/CONTRIBUTING.html","id":"pull-request-process","dir":"","previous_headings":"Bigger changes","what":"Pull request process","title":"Contributing to peskas.kenya.data.pipeline","text":"Fork package clone onto computer. haven’t done , recommend using usethis::create_from_github(\"WorldFishCenter/peskas.kenya.data.pipeline\", fork = TRUE). Install development dependencies devtools::install_dev_deps(), make sure package passes R CMD check running devtools::check(). R CMD check doesn’t pass cleanly, ’s good idea ask help continuing. Create Git branch pull request (PR). recommend using usethis::pr_init(\"brief-description--change\"). Make changes, commit git, create PR running usethis::pr_push(), following prompts browser. title PR briefly describe change. body PR contain Fixes #issue-number. user-facing changes, add bullet top NEWS.md (.e. just first header). Follow style described https://style.tidyverse.org/news.html.","code":""},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/CONTRIBUTING.html","id":"code-style","dir":"","previous_headings":"Bigger changes","what":"Code style","title":"Contributing to peskas.kenya.data.pipeline","text":"New code follow tidyverse style guide. can use styler package apply styles, please don’t restyle code nothing PR. use roxygen2, Markdown syntax, documentation. use testthat unit tests. Contributions test cases included easier accept.","code":""},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/CONTRIBUTING.html","id":"code-of-conduct","dir":"","previous_headings":"","what":"Code of Conduct","title":"Contributing to peskas.kenya.data.pipeline","text":"Please note peskas.kenya.data.pipeline project released Contributor Code Conduct. contributing project agree abide terms.","code":""},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/LICENSE.html","id":null,"dir":"","previous_headings":"","what":"GNU General Public License","title":"GNU General Public License","text":"Version 3, 29 June 2007Copyright © 2007 Free Software Foundation, Inc. <http://fsf.org/> Everyone permitted copy distribute verbatim copies license document, changing allowed.","code":""},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/LICENSE.html","id":"preamble","dir":"","previous_headings":"","what":"Preamble","title":"GNU General Public License","text":"GNU General Public License free, copyleft license software kinds works. licenses software practical works designed take away freedom share change works. contrast, GNU General Public License intended guarantee freedom share change versions program–make sure remains free software users. , Free Software Foundation, use GNU General Public License software; applies also work released way authors. can apply programs, . speak free software, referring freedom, price. General Public Licenses designed make sure freedom distribute copies free software (charge wish), receive source code can get want , can change software use pieces new free programs, know can things. protect rights, need prevent others denying rights asking surrender rights. Therefore, certain responsibilities distribute copies software, modify : responsibilities respect freedom others. example, distribute copies program, whether gratis fee, must pass recipients freedoms received. must make sure , , receive can get source code. must show terms know rights. Developers use GNU GPL protect rights two steps: (1) assert copyright software, (2) offer License giving legal permission copy, distribute /modify . developers’ authors’ protection, GPL clearly explains warranty free software. users’ authors’ sake, GPL requires modified versions marked changed, problems attributed erroneously authors previous versions. devices designed deny users access install run modified versions software inside , although manufacturer can . fundamentally incompatible aim protecting users’ freedom change software. systematic pattern abuse occurs area products individuals use, precisely unacceptable. Therefore, designed version GPL prohibit practice products. problems arise substantially domains, stand ready extend provision domains future versions GPL, needed protect freedom users. Finally, every program threatened constantly software patents. States allow patents restrict development use software general-purpose computers, , wish avoid special danger patents applied free program make effectively proprietary. prevent , GPL assures patents used render program non-free. precise terms conditions copying, distribution modification follow.","code":""},{"path":[]},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/LICENSE.html","id":"id_0-definitions","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"0. Definitions","title":"GNU General Public License","text":"“License” refers version 3 GNU General Public License. “Copyright” also means copyright-like laws apply kinds works, semiconductor masks. “Program” refers copyrightable work licensed License. licensee addressed “”. “Licensees” “recipients” may individuals organizations. “modify” work means copy adapt part work fashion requiring copyright permission, making exact copy. resulting work called “modified version” earlier work work “based ” earlier work. “covered work” means either unmodified Program work based Program. “propagate” work means anything , without permission, make directly secondarily liable infringement applicable copyright law, except executing computer modifying private copy. Propagation includes copying, distribution (without modification), making available public, countries activities well. “convey” work means kind propagation enables parties make receive copies. Mere interaction user computer network, transfer copy, conveying. interactive user interface displays “Appropriate Legal Notices” extent includes convenient prominently visible feature (1) displays appropriate copyright notice, (2) tells user warranty work (except extent warranties provided), licensees may convey work License, view copy License. interface presents list user commands options, menu, prominent item list meets criterion.","code":""},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/LICENSE.html","id":"id_1-source-code","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"1. Source Code","title":"GNU General Public License","text":"“source code” work means preferred form work making modifications . “Object code” means non-source form work. “Standard Interface” means interface either official standard defined recognized standards body, , case interfaces specified particular programming language, one widely used among developers working language. “System Libraries” executable work include anything, work whole, () included normal form packaging Major Component, part Major Component, (b) serves enable use work Major Component, implement Standard Interface implementation available public source code form. “Major Component”, context, means major essential component (kernel, window system, ) specific operating system () executable work runs, compiler used produce work, object code interpreter used run . “Corresponding Source” work object code form means source code needed generate, install, (executable work) run object code modify work, including scripts control activities. However, include work’s System Libraries, general-purpose tools generally available free programs used unmodified performing activities part work. example, Corresponding Source includes interface definition files associated source files work, source code shared libraries dynamically linked subprograms work specifically designed require, intimate data communication control flow subprograms parts work. Corresponding Source need include anything users can regenerate automatically parts Corresponding Source. Corresponding Source work source code form work.","code":""},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/LICENSE.html","id":"id_2-basic-permissions","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"2. Basic Permissions","title":"GNU General Public License","text":"rights granted License granted term copyright Program, irrevocable provided stated conditions met. License explicitly affirms unlimited permission run unmodified Program. output running covered work covered License output, given content, constitutes covered work. License acknowledges rights fair use equivalent, provided copyright law. may make, run propagate covered works convey, without conditions long license otherwise remains force. may convey covered works others sole purpose make modifications exclusively , provide facilities running works, provided comply terms License conveying material control copyright. thus making running covered works must exclusively behalf, direction control, terms prohibit making copies copyrighted material outside relationship . Conveying circumstances permitted solely conditions stated . Sublicensing allowed; section 10 makes unnecessary.","code":""},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/LICENSE.html","id":"id_3-protecting-users-legal-rights-from-anti-circumvention-law","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"3. Protecting Users’ Legal Rights From Anti-Circumvention Law","title":"GNU General Public License","text":"covered work shall deemed part effective technological measure applicable law fulfilling obligations article 11 WIPO copyright treaty adopted 20 December 1996, similar laws prohibiting restricting circumvention measures. convey covered work, waive legal power forbid circumvention technological measures extent circumvention effected exercising rights License respect covered work, disclaim intention limit operation modification work means enforcing, work’s users, third parties’ legal rights forbid circumvention technological measures.","code":""},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/LICENSE.html","id":"id_4-conveying-verbatim-copies","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"4. Conveying Verbatim Copies","title":"GNU General Public License","text":"may convey verbatim copies Program’s source code receive , medium, provided conspicuously appropriately publish copy appropriate copyright notice; keep intact notices stating License non-permissive terms added accord section 7 apply code; keep intact notices absence warranty; give recipients copy License along Program. may charge price price copy convey, may offer support warranty protection fee.","code":""},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/LICENSE.html","id":"id_5-conveying-modified-source-versions","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"5. Conveying Modified Source Versions","title":"GNU General Public License","text":"may convey work based Program, modifications produce Program, form source code terms section 4, provided also meet conditions: ) work must carry prominent notices stating modified , giving relevant date. b) work must carry prominent notices stating released License conditions added section 7. requirement modifies requirement section 4 “keep intact notices”. c) must license entire work, whole, License anyone comes possession copy. License therefore apply, along applicable section 7 additional terms, whole work, parts, regardless packaged. License gives permission license work way, invalidate permission separately received . d) work interactive user interfaces, must display Appropriate Legal Notices; however, Program interactive interfaces display Appropriate Legal Notices, work need make . compilation covered work separate independent works, nature extensions covered work, combined form larger program, volume storage distribution medium, called “aggregate” compilation resulting copyright used limit access legal rights compilation’s users beyond individual works permit. Inclusion covered work aggregate cause License apply parts aggregate.","code":""},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/LICENSE.html","id":"id_6-conveying-non-source-forms","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"6. Conveying Non-Source Forms","title":"GNU General Public License","text":"may convey covered work object code form terms sections 4 5, provided also convey machine-readable Corresponding Source terms License, one ways: ) Convey object code , embodied , physical product (including physical distribution medium), accompanied Corresponding Source fixed durable physical medium customarily used software interchange. b) Convey object code , embodied , physical product (including physical distribution medium), accompanied written offer, valid least three years valid long offer spare parts customer support product model, give anyone possesses object code either (1) copy Corresponding Source software product covered License, durable physical medium customarily used software interchange, price reasonable cost physically performing conveying source, (2) access copy Corresponding Source network server charge. c) Convey individual copies object code copy written offer provide Corresponding Source. alternative allowed occasionally noncommercially, received object code offer, accord subsection 6b. d) Convey object code offering access designated place (gratis charge), offer equivalent access Corresponding Source way place charge. need require recipients copy Corresponding Source along object code. place copy object code network server, Corresponding Source may different server (operated third party) supports equivalent copying facilities, provided maintain clear directions next object code saying find Corresponding Source. Regardless server hosts Corresponding Source, remain obligated ensure available long needed satisfy requirements. e) Convey object code using peer--peer transmission, provided inform peers object code Corresponding Source work offered general public charge subsection 6d. separable portion object code, whose source code excluded Corresponding Source System Library, need included conveying object code work. “User Product” either (1) “consumer product”, means tangible personal property normally used personal, family, household purposes, (2) anything designed sold incorporation dwelling. determining whether product consumer product, doubtful cases shall resolved favor coverage. particular product received particular user, “normally used” refers typical common use class product, regardless status particular user way particular user actually uses, expects expected use, product. product consumer product regardless whether product substantial commercial, industrial non-consumer uses, unless uses represent significant mode use product. “Installation Information” User Product means methods, procedures, authorization keys, information required install execute modified versions covered work User Product modified version Corresponding Source. information must suffice ensure continued functioning modified object code case prevented interfered solely modification made. convey object code work section , , specifically use , User Product, conveying occurs part transaction right possession use User Product transferred recipient perpetuity fixed term (regardless transaction characterized), Corresponding Source conveyed section must accompanied Installation Information. requirement apply neither third party retains ability install modified object code User Product (example, work installed ROM). requirement provide Installation Information include requirement continue provide support service, warranty, updates work modified installed recipient, User Product modified installed. Access network may denied modification materially adversely affects operation network violates rules protocols communication across network. Corresponding Source conveyed, Installation Information provided, accord section must format publicly documented (implementation available public source code form), must require special password key unpacking, reading copying.","code":""},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/LICENSE.html","id":"id_7-additional-terms","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"7. Additional Terms","title":"GNU General Public License","text":"“Additional permissions” terms supplement terms License making exceptions one conditions. Additional permissions applicable entire Program shall treated though included License, extent valid applicable law. additional permissions apply part Program, part may used separately permissions, entire Program remains governed License without regard additional permissions. convey copy covered work, may option remove additional permissions copy, part . (Additional permissions may written require removal certain cases modify work.) may place additional permissions material, added covered work, can give appropriate copyright permission. Notwithstanding provision License, material add covered work, may (authorized copyright holders material) supplement terms License terms: ) Disclaiming warranty limiting liability differently terms sections 15 16 License; b) Requiring preservation specified reasonable legal notices author attributions material Appropriate Legal Notices displayed works containing ; c) Prohibiting misrepresentation origin material, requiring modified versions material marked reasonable ways different original version; d) Limiting use publicity purposes names licensors authors material; e) Declining grant rights trademark law use trade names, trademarks, service marks; f) Requiring indemnification licensors authors material anyone conveys material (modified versions ) contractual assumptions liability recipient, liability contractual assumptions directly impose licensors authors. non-permissive additional terms considered “restrictions” within meaning section 10. Program received , part , contains notice stating governed License along term restriction, may remove term. license document contains restriction permits relicensing conveying License, may add covered work material governed terms license document, provided restriction survive relicensing conveying. add terms covered work accord section, must place, relevant source files, statement additional terms apply files, notice indicating find applicable terms. Additional terms, permissive non-permissive, may stated form separately written license, stated exceptions; requirements apply either way.","code":""},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/LICENSE.html","id":"id_8-termination","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"8. Termination","title":"GNU General Public License","text":"may propagate modify covered work except expressly provided License. attempt otherwise propagate modify void, automatically terminate rights License (including patent licenses granted third paragraph section 11). However, cease violation License, license particular copyright holder reinstated () provisionally, unless copyright holder explicitly finally terminates license, (b) permanently, copyright holder fails notify violation reasonable means prior 60 days cessation. Moreover, license particular copyright holder reinstated permanently copyright holder notifies violation reasonable means, first time received notice violation License (work) copyright holder, cure violation prior 30 days receipt notice. Termination rights section terminate licenses parties received copies rights License. rights terminated permanently reinstated, qualify receive new licenses material section 10.","code":""},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/LICENSE.html","id":"id_9-acceptance-not-required-for-having-copies","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"9. Acceptance Not Required for Having Copies","title":"GNU General Public License","text":"required accept License order receive run copy Program. Ancillary propagation covered work occurring solely consequence using peer--peer transmission receive copy likewise require acceptance. However, nothing License grants permission propagate modify covered work. actions infringe copyright accept License. Therefore, modifying propagating covered work, indicate acceptance License .","code":""},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/LICENSE.html","id":"id_10-automatic-licensing-of-downstream-recipients","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"10. Automatic Licensing of Downstream Recipients","title":"GNU General Public License","text":"time convey covered work, recipient automatically receives license original licensors, run, modify propagate work, subject License. responsible enforcing compliance third parties License. “entity transaction” transaction transferring control organization, substantially assets one, subdividing organization, merging organizations. propagation covered work results entity transaction, party transaction receives copy work also receives whatever licenses work party’s predecessor interest give previous paragraph, plus right possession Corresponding Source work predecessor interest, predecessor can get reasonable efforts. may impose restrictions exercise rights granted affirmed License. example, may impose license fee, royalty, charge exercise rights granted License, may initiate litigation (including cross-claim counterclaim lawsuit) alleging patent claim infringed making, using, selling, offering sale, importing Program portion .","code":""},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/LICENSE.html","id":"id_11-patents","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"11. Patents","title":"GNU General Public License","text":"“contributor” copyright holder authorizes use License Program work Program based. work thus licensed called contributor’s “contributor version”. contributor’s “essential patent claims” patent claims owned controlled contributor, whether already acquired hereafter acquired, infringed manner, permitted License, making, using, selling contributor version, include claims infringed consequence modification contributor version. purposes definition, “control” includes right grant patent sublicenses manner consistent requirements License. contributor grants non-exclusive, worldwide, royalty-free patent license contributor’s essential patent claims, make, use, sell, offer sale, import otherwise run, modify propagate contents contributor version. following three paragraphs, “patent license” express agreement commitment, however denominated, enforce patent (express permission practice patent covenant sue patent infringement). “grant” patent license party means make agreement commitment enforce patent party. convey covered work, knowingly relying patent license, Corresponding Source work available anyone copy, free charge terms License, publicly available network server readily accessible means, must either (1) cause Corresponding Source available, (2) arrange deprive benefit patent license particular work, (3) arrange, manner consistent requirements License, extend patent license downstream recipients. “Knowingly relying” means actual knowledge , patent license, conveying covered work country, recipient’s use covered work country, infringe one identifiable patents country reason believe valid. , pursuant connection single transaction arrangement, convey, propagate procuring conveyance , covered work, grant patent license parties receiving covered work authorizing use, propagate, modify convey specific copy covered work, patent license grant automatically extended recipients covered work works based . patent license “discriminatory” include within scope coverage, prohibits exercise , conditioned non-exercise one rights specifically granted License. may convey covered work party arrangement third party business distributing software, make payment third party based extent activity conveying work, third party grants, parties receive covered work , discriminatory patent license () connection copies covered work conveyed (copies made copies), (b) primarily connection specific products compilations contain covered work, unless entered arrangement, patent license granted, prior 28 March 2007. Nothing License shall construed excluding limiting implied license defenses infringement may otherwise available applicable patent law.","code":""},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/LICENSE.html","id":"id_12-no-surrender-of-others-freedom","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"12. No Surrender of Others’ Freedom","title":"GNU General Public License","text":"conditions imposed (whether court order, agreement otherwise) contradict conditions License, excuse conditions License. convey covered work satisfy simultaneously obligations License pertinent obligations, consequence may convey . example, agree terms obligate collect royalty conveying convey Program, way satisfy terms License refrain entirely conveying Program.","code":""},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/LICENSE.html","id":"id_13-use-with-the-gnu-affero-general-public-license","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"13. Use with the GNU Affero General Public License","title":"GNU General Public License","text":"Notwithstanding provision License, permission link combine covered work work licensed version 3 GNU Affero General Public License single combined work, convey resulting work. terms License continue apply part covered work, special requirements GNU Affero General Public License, section 13, concerning interaction network apply combination .","code":""},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/LICENSE.html","id":"id_14-revised-versions-of-this-license","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"14. Revised Versions of this License","title":"GNU General Public License","text":"Free Software Foundation may publish revised /new versions GNU General Public License time time. new versions similar spirit present version, may differ detail address new problems concerns. version given distinguishing version number. Program specifies certain numbered version GNU General Public License “later version” applies , option following terms conditions either numbered version later version published Free Software Foundation. Program specify version number GNU General Public License, may choose version ever published Free Software Foundation. Program specifies proxy can decide future versions GNU General Public License can used, proxy’s public statement acceptance version permanently authorizes choose version Program. Later license versions may give additional different permissions. However, additional obligations imposed author copyright holder result choosing follow later version.","code":""},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/LICENSE.html","id":"id_15-disclaimer-of-warranty","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"15. Disclaimer of Warranty","title":"GNU General Public License","text":"WARRANTY PROGRAM, EXTENT PERMITTED APPLICABLE LAW. EXCEPT OTHERWISE STATED WRITING COPYRIGHT HOLDERS /PARTIES PROVIDE PROGRAM “” WITHOUT WARRANTY KIND, EITHER EXPRESSED IMPLIED, INCLUDING, LIMITED , IMPLIED WARRANTIES MERCHANTABILITY FITNESS PARTICULAR PURPOSE. ENTIRE RISK QUALITY PERFORMANCE PROGRAM . PROGRAM PROVE DEFECTIVE, ASSUME COST NECESSARY SERVICING, REPAIR CORRECTION.","code":""},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/LICENSE.html","id":"id_16-limitation-of-liability","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"16. Limitation of Liability","title":"GNU General Public License","text":"EVENT UNLESS REQUIRED APPLICABLE LAW AGREED WRITING COPYRIGHT HOLDER, PARTY MODIFIES /CONVEYS PROGRAM PERMITTED , LIABLE DAMAGES, INCLUDING GENERAL, SPECIAL, INCIDENTAL CONSEQUENTIAL DAMAGES ARISING USE INABILITY USE PROGRAM (INCLUDING LIMITED LOSS DATA DATA RENDERED INACCURATE LOSSES SUSTAINED THIRD PARTIES FAILURE PROGRAM OPERATE PROGRAMS), EVEN HOLDER PARTY ADVISED POSSIBILITY DAMAGES.","code":""},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/LICENSE.html","id":"id_17-interpretation-of-sections-15-and-16","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"17. Interpretation of Sections 15 and 16","title":"GNU General Public License","text":"disclaimer warranty limitation liability provided given local legal effect according terms, reviewing courts shall apply local law closely approximates absolute waiver civil liability connection Program, unless warranty assumption liability accompanies copy Program return fee. END TERMS CONDITIONS","code":""},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/LICENSE.html","id":"how-to-apply-these-terms-to-your-new-programs","dir":"","previous_headings":"","what":"How to Apply These Terms to Your New Programs","title":"GNU General Public License","text":"develop new program, want greatest possible use public, best way achieve make free software everyone can redistribute change terms. , attach following notices program. safest attach start source file effectively state exclusion warranty; file least “copyright” line pointer full notice found. Also add information contact electronic paper mail. program terminal interaction, make output short notice like starts interactive mode: hypothetical commands show w show c show appropriate parts General Public License. course, program’s commands might different; GUI interface, use “box”. also get employer (work programmer) school, , sign “copyright disclaimer” program, necessary. information , apply follow GNU GPL, see <http://www.gnu.org/licenses/>. GNU General Public License permit incorporating program proprietary programs. program subroutine library, may consider useful permit linking proprietary applications library. want , use GNU Lesser General Public License instead License. first, please read <http://www.gnu.org/philosophy/--lgpl.html>.","code":"<one line to give the program's name and a brief idea of what it does.> Copyright (C) <year>  <name of author>  This program is free software: you can redistribute it and/or modify it under the terms of the GNU General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option) any later version.  This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License for more details.  You should have received a copy of the GNU General Public License along with this program.  If not, see <http://www.gnu.org/licenses/>. <program>  Copyright (C) <year>  <name of author> This program comes with ABSOLUTELY NO WARRANTY; for details type 'show w'. This is free software, and you are welcome to redistribute it under certain conditions; type 'show c' for details."},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/SUPPORT.html","id":null,"dir":"","previous_headings":"","what":"Getting help with peskas.kenya.data.pipeline","title":"Getting help with peskas.kenya.data.pipeline","text":"Thanks using peskas.kenya.data.pipeline! filing issue, places explore pieces put together make process smooth possible.","code":""},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/SUPPORT.html","id":"make-a-reprex","dir":"","previous_headings":"","what":"Make a reprex","title":"Getting help with peskas.kenya.data.pipeline","text":"Start making minimal reproducible example using reprex package. haven’t heard used reprex , ’re treat! Seriously, reprex make R-question-asking endeavors easier (pretty incredible ROI five ten minutes ’ll take learn ’s ). additional reprex pointers, check Get help! section tidyverse site.","code":""},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/SUPPORT.html","id":"where-to-ask","dir":"","previous_headings":"","what":"Where to ask?","title":"Getting help with peskas.kenya.data.pipeline","text":"Armed reprex, next step figure ask. ’s question: start community.rstudio.com, /StackOverflow. people answer questions. ’s bug: ’re right place, file issue. ’re sure: let community help figure ! problem bug feature request, can easily return report . opening new issue, sure search issues pull requests make sure bug hasn’t reported /already fixed development version. default, search pre-populated :issue :open. can edit qualifiers (e.g. :pr, :closed) needed. example, ’d simply remove :open search issues repo, open closed.","code":""},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/SUPPORT.html","id":"what-happens-next","dir":"","previous_headings":"","what":"What happens next?","title":"Getting help with peskas.kenya.data.pipeline","text":"efficient possible, development tidyverse packages tends bursty, shouldn’t worry don’t get immediate response. Typically don’t look repo sufficient quantity issues accumulates, ’s burst intense activity focus efforts. makes development efficient avoids expensive context switching problems, cost taking longer get back . process makes good reprex particularly important might multiple months initial report start working . can’t reproduce bug, can’t fix !","code":""},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/authors.html","id":null,"dir":"","previous_headings":"","what":"Authors","title":"Authors and Citation","text":"Lorenzo Longobardi. Author, maintainer. Hamza Altarturi. Contributor. WorldFish. Copyright holder.","code":""},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/authors.html","id":"citation","dir":"","previous_headings":"","what":"Citation","title":"Authors and Citation","text":"Longobardi L (2025). peskas.kenya.data.pipeline: Kenya Small-Scale Fisheries Data Pipeline. R package version 4.3.0, https://worldfishcenter.github.io/peskas.kenya.data.pipeline/.","code":"@Manual{,   title = {peskas.kenya.data.pipeline: Kenya Small-Scale Fisheries Data Pipeline},   author = {Lorenzo Longobardi},   year = {2025},   note = {R package version 4.3.0},   url = {https://worldfishcenter.github.io/peskas.kenya.data.pipeline/}, }"},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/index.html","id":"peskaskenyadatapipeline-","dir":"","previous_headings":"","what":"Kenya Small-Scale Fisheries Data Pipeline","title":"Kenya Small-Scale Fisheries Data Pipeline","text":"goal peskas.kenya.data.pipeline implement, deploy, execute data modelling pipelines underpin Peskas Kenya, partnership WorldFish Wildlife Conservation Society part Asia-Africa Bluetech Superhighway project funded FCDO UK Government.","code":""},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/index.html","id":"the-pipeline-is-an-r-package","dir":"","previous_headings":"","what":"The pipeline is an R package","title":"Kenya Small-Scale Fisheries Data Pipeline","text":"peskas.kenya.data.pipeline structured R package makes easier write production-grade software. Specifically, structuring code R package allows us : better handle system package dependencies, forces us split code functions, makes easier document code, makes easier test code make heavy use tidyverse style conventions usethis package automate tasks project setup deployment. information rationale structuring pipeline package check Chapter 3 Engineering Production-Grade Shiny Apps. book focused Shiny applications rationale also applies data pipelines production-ready code general.","code":""},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/index.html","id":"how-the-pipeline-works","dir":"","previous_headings":"","what":"How the pipeline works","title":"Kenya Small-Scale Fisheries Data Pipeline","text":"pipeline composed different modules: Data Collection: site fishing landing surveys continuous, solar-powered GPS vessel trackers collect send data near real-time, alongside fishery metadata thorough data-gathering process. Pre-processing: Data formatting, shaping, standardisation prepare raw data analysis. Validation: Outlier detection error identification, includes alert system maintain data quality. Analytics: Modelling fisheries indicators, nutritional characterization, data mining extract valuable insights. Data export: Automated dissemination processed analysed fisheries data ensure accessibility comprehension. involves restructuring data dashboard integration open publication. isualisation: Tools data reporting sharing insights comprehensive dedicated web app dashboard (hosted repository). See Peskas: Automated analytics small-scale, data-deficient fisheries details.","code":""},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/index.html","id":"getting-started","dir":"","previous_headings":"","what":"Getting Started","title":"Kenya Small-Scale Fisheries Data Pipeline","text":"package uses configuration file config.yml manage environment-specific settings connections. get started, familiarize package structure, particularly R directory main functions located. function typically reads configuration using read_config() access necessary parameters. work package locally, ’ll need set environment variables using .env file. Copy .env.example .env fill actual credentials. package automatically load environment variables reading configuration. Remember run devtools::load_all() testing changes locally. ’re new R package development, consider reviewing R packages book Hadley Wickham Jenny Brian.","code":""},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/index.html","id":"quick-guide-for-contributors","dir":"","previous_headings":"","what":"Quick Guide for Contributors","title":"Kenya Small-Scale Fisheries Data Pipeline","text":"keep repository clean efficient, please keep guidelines mind: Always work new branch, directly main. Write clear, concise commit messages. Avoid storing intermediate garbage files, especially root folder. Strive soft-coded solutions. Maintain consistent code style throughout project. Document code well - future (others) thank . Test changes thoroughly submitting pull request. Keep fork synced main repository. practices help us maintain clean, efficient codebase ’s easier everyone work . detailed guidelines, check CONTRIBUTING.md file.","code":""},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/reference/add_version.html","id":null,"dir":"Reference","previous_headings":"","what":"Add timestamp and sha string to a file name — add_version","title":"Add timestamp and sha string to a file name — add_version","text":"alternative version data name using sha (unique identifier) code using generate process data time data generated processed. function adds information, version identifier, file name (character string)","code":""},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/reference/add_version.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Add timestamp and sha string to a file name — add_version","text":"","code":"add_version(filename, extension = \"\", sha_nchar = 7, sep = \"__\")"},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/reference/add_version.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Add timestamp and sha string to a file name — add_version","text":"filename Path sans extension file version extension Extension file sha_nchar Number characters SHA use version identifier sep Characters separating version identifier file name","code":""},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/reference/add_version.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Add timestamp and sha string to a file name — add_version","text":"character string file name version identifier","code":""},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/reference/add_version.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Add timestamp and sha string to a file name — add_version","text":"SHA information retrieved using git2r::sha. code running context aware git repository (example code running inside container) function attempts get sha environment variable GITHUB_SHA. methods fail, sha versioning added.","code":""},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/reference/add_version.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Add timestamp and sha string to a file name — add_version","text":"","code":"if (git2r::in_repository()) {   add_version(\"my_file\", \"csv\") } #> [1] \"my_file__20251103061136_5de08c7__.csv\""},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/reference/airtable_to_df.html","id":null,"dir":"Reference","previous_headings":"","what":"Get All Records from Airtable with Pagination — airtable_to_df","title":"Get All Records from Airtable with Pagination — airtable_to_df","text":"Retrieves records Airtable table, handling pagination automatically. Retrieves records Airtable table, handling pagination automatically.","code":""},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/reference/airtable_to_df.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get All Records from Airtable with Pagination — airtable_to_df","text":"","code":"airtable_to_df(base_id, table_name, token, list_handler = \"collapse\")  airtable_to_df(base_id, table_name, token, list_handler = \"collapse\")"},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/reference/airtable_to_df.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get All Records from Airtable with Pagination — airtable_to_df","text":"base_id Character string. Airtable base ID. table_name Character string. name table retrieve. token Character string. Airtable API token authentication. list_handler Character string. \"collapse\" (default) \"count\" list fields.","code":""},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/reference/airtable_to_df.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Get All Records from Airtable with Pagination — airtable_to_df","text":"tibble records 'airtable_id' column. tibble records 'airtable_id' column.","code":""},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/reference/alert_outlier.html","id":null,"dir":"Reference","previous_headings":"","what":"Generate an alert vector based on the univOutl::LocScaleB() function — alert_outlier","title":"Generate an alert vector based on the univOutl::LocScaleB() function — alert_outlier","text":"Generate alert vector based univOutl::LocScaleB() function","code":""},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/reference/alert_outlier.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Generate an alert vector based on the univOutl::LocScaleB() function — alert_outlier","text":"","code":"alert_outlier(   x,   no_alert_value = NA_real_,   alert_if_larger = no_alert_value,   alert_if_smaller = no_alert_value,   ... )"},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/reference/alert_outlier.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Generate an alert vector based on the univOutl::LocScaleB() function — alert_outlier","text":"x numeric vector outliers checked no_alert_value value put output alert (x within bounds) alert_if_larger alert x bounds found univOutl::LocScaleB() alert_if_smaller alert x bounds found univOutl::LocScaleB() ... arguments univOutl::LocScaleB()","code":""},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/reference/alert_outlier.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Generate an alert vector based on the univOutl::LocScaleB() function — alert_outlier","text":"vector lenght x","code":""},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/reference/alert_outlier_iqr.html","id":null,"dir":"Reference","previous_headings":"","what":"Generate an alert vector based on IQR method — alert_outlier_iqr","title":"Generate an alert vector based on IQR method — alert_outlier_iqr","text":"Generate alert vector based IQR method","code":""},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/reference/alert_outlier_iqr.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Generate an alert vector based on IQR method — alert_outlier_iqr","text":"","code":"alert_outlier_iqr(   x,   no_alert_value = NA_real_,   alert_if_larger = no_alert_value,   alert_if_smaller = no_alert_value,   multiplier = 1.5 )"},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/reference/alert_outlier_iqr.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Generate an alert vector based on IQR method — alert_outlier_iqr","text":"x numeric vector outliers checked no_alert_value value put output alert alert_if_larger alert x upper bound alert_if_smaller alert x lower bound multiplier multiplier IQR range (default 1.5)","code":""},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/reference/alert_outlier_iqr.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Generate an alert vector based on IQR method — alert_outlier_iqr","text":"vector length x alert values","code":""},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/reference/apply_gear_mapping_v1.html","id":null,"dir":"Reference","previous_headings":"","what":"Gear mapping for version 1 data — apply_gear_mapping_v1","title":"Gear mapping for version 1 data — apply_gear_mapping_v1","text":"Gear mapping version 1 data","code":""},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/reference/apply_gear_mapping_v1.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Gear mapping for version 1 data — apply_gear_mapping_v1","text":"","code":"apply_gear_mapping_v1()"},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/reference/apply_gear_mapping_v2.html","id":null,"dir":"Reference","previous_headings":"","what":"Gear mapping for version 2 data — apply_gear_mapping_v2","title":"Gear mapping for version 2 data — apply_gear_mapping_v2","text":"Gear mapping version 2 data","code":""},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/reference/apply_gear_mapping_v2.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Gear mapping for version 2 data — apply_gear_mapping_v2","text":"","code":"apply_gear_mapping_v2()"},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/reference/bulk_update_airtable.html","id":null,"dir":"Reference","previous_headings":"","what":"Bulk Update Multiple Airtable Records — bulk_update_airtable","title":"Bulk Update Multiple Airtable Records — bulk_update_airtable","text":"Updates multiple records batches 10.","code":""},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/reference/bulk_update_airtable.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Bulk Update Multiple Airtable Records — bulk_update_airtable","text":"","code":"bulk_update_airtable(base_id, table_name, token, updates_df)"},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/reference/bulk_update_airtable.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Bulk Update Multiple Airtable Records — bulk_update_airtable","text":"base_id Character string. Airtable base ID. table_name Character string. Name table. token Character string. Airtable API token. updates_df Data frame 'airtable_id' column fields update.","code":""},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/reference/bulk_update_airtable.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Bulk Update Multiple Airtable Records — bulk_update_airtable","text":"List response objects.","code":""},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/reference/check_outliers_iqr.html","id":null,"dir":"Reference","previous_headings":"","what":"Check for outliers using IQR method — check_outliers_iqr","title":"Check for outliers using IQR method — check_outliers_iqr","text":"Check outliers using IQR method","code":""},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/reference/check_outliers_iqr.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Check for outliers using IQR method — check_outliers_iqr","text":"","code":"check_outliers_iqr(x, multiplier = 1.5)"},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/reference/check_outliers_iqr.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Check for outliers using IQR method — check_outliers_iqr","text":"x numeric vector outliers checked multiplier multiplier IQR range (default 1.5)","code":""},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/reference/check_outliers_iqr.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Check for outliers using IQR method — check_outliers_iqr","text":"logical vector indicating values within bounds (TRUE) outliers (FALSE)","code":""},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/reference/check_outliers_iqr.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Check for outliers using IQR method — check_outliers_iqr","text":"","code":"if (FALSE) { # \\dontrun{ x <- c(1, 2, 3, 100) check_outliers_iqr(x, multiplier = 1.5) } # }"},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/reference/clean_catch_names.html","id":null,"dir":"Reference","previous_headings":"","what":"Clean Catch Names — clean_catch_names","title":"Clean Catch Names — clean_catch_names","text":"function standardizes catch names dataset correcting common misspellings inconsistencies.","code":""},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/reference/clean_catch_names.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Clean Catch Names — clean_catch_names","text":"","code":"clean_catch_names(data = NULL)"},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/reference/clean_catch_names.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Clean Catch Names — clean_catch_names","text":"data data frame tibble containing column named catch_name.","code":""},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/reference/clean_catch_names.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Clean Catch Names — clean_catch_names","text":"data frame tibble standardized catch names catch_name column.","code":""},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/reference/clean_catch_names.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Clean Catch Names — clean_catch_names","text":"","code":"if (FALSE) { # \\dontrun{ cleaned_data <- clean_catch_names(data) } # }"},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/reference/cloud_object_name.html","id":null,"dir":"Reference","previous_headings":"","what":"Retrieve Full Name of Versioned Cloud Object — cloud_object_name","title":"Retrieve Full Name of Versioned Cloud Object — cloud_object_name","text":"Gets full name(s) object(s) cloud storage matching specified prefix, version, file extension.","code":""},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/reference/cloud_object_name.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Retrieve Full Name of Versioned Cloud Object — cloud_object_name","text":"","code":"cloud_object_name(   prefix,   version = \"latest\",   extension = \"\",   provider,   exact_match = FALSE,   options )"},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/reference/cloud_object_name.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Retrieve Full Name of Versioned Cloud Object — cloud_object_name","text":"prefix string indicating object's prefix. version string specifying version (\"latest\" specific version string). extension file extension filter . empty string (\"\") includes extensions. provider character string specifying cloud provider (\"gcs\" \"aws\"). exact_match logical indicating whether match prefix exactly. options named list provider-specific options including bucket authentication details.","code":""},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/reference/cloud_object_name.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Retrieve Full Name of Versioned Cloud Object — cloud_object_name","text":"vector names objects matching criteria.","code":""},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/reference/cloud_object_name.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Retrieve Full Name of Versioned Cloud Object — cloud_object_name","text":"GCS, options list include: bucket: bucket name. service_account_key: authentication JSON contents, previously authenticated.","code":""},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/reference/cloud_object_name.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Retrieve Full Name of Versioned Cloud Object — cloud_object_name","text":"","code":"if (FALSE) { # \\dontrun{ authentication_details <- readLines(\"path/to/json_file.json\") cloud_object_name(   \"prefix\",   \"latest\",   \"json\",   \"gcs\",   list(service_account_key = authentication_details, bucket = \"my-bucket\") ) #' } # }"},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/reference/cloud_storage_authenticate.html","id":null,"dir":"Reference","previous_headings":"","what":"Authenticate to a Cloud Storage Provider — cloud_storage_authenticate","title":"Authenticate to a Cloud Storage Provider — cloud_storage_authenticate","text":"function primarily used internally functions establish authentication specified cloud providers Google Cloud Services (GCS) Amazon Web Services (AWS).","code":""},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/reference/cloud_storage_authenticate.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Authenticate to a Cloud Storage Provider — cloud_storage_authenticate","text":"","code":"cloud_storage_authenticate(provider, options)"},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/reference/cloud_storage_authenticate.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Authenticate to a Cloud Storage Provider — cloud_storage_authenticate","text":"provider character string specifying cloud provider (\"gcs\" \"aws\"). options named list options specific cloud provider (see details).","code":""},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/reference/cloud_storage_authenticate.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Authenticate to a Cloud Storage Provider — cloud_storage_authenticate","text":"GCS, options list must include: service_account_key: contents authentication JSON file Google Project. function wraps googleCloudStorageR::gcs_auth() handle GCS authentication.","code":""},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/reference/cloud_storage_authenticate.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Authenticate to a Cloud Storage Provider — cloud_storage_authenticate","text":"","code":"if (FALSE) { # \\dontrun{ authentication_details <- readLines(\"path/to/json_file.json\") cloud_storage_authenticate(\"gcs\", list(service_account_key = authentication_details)) #' } # }"},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/reference/create_geos.html","id":null,"dir":"Reference","previous_headings":"","what":"Generate Geographic Regional Summaries of Fishery Data — create_geos","title":"Generate Geographic Regional Summaries of Fishery Data — create_geos","text":"function creates geospatial representations fishery metrics aggregating BMU (Beach Management Unit) data regional levels along Kenyan coast. assigns BMU nearest coastal region, calculates regional summaries fishery performance metrics, exports results GeoJSON file spatial visualization.","code":""},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/reference/create_geos.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Generate Geographic Regional Summaries of Fishery Data — create_geos","text":"","code":"create_geos(monthly_summaries_dat = NULL, conf = conf)"},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/reference/create_geos.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Generate Geographic Regional Summaries of Fishery Data — create_geos","text":"monthly_summaries_dat data frame containing monthly fishery metrics BMU, typically output get_fishery_metrics() function, columns: BMU, date, mean_effort, mean_cpue, mean_cpua, mean_rpue, mean_rpua. conf configuration object containing path GeoJSON file \"KEN_coast_regions.geojson\" included peskas.kenya.data.pipeline package","code":""},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/reference/create_geos.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Generate Geographic Regional Summaries of Fishery Data — create_geos","text":"function return value. writes GeoJSON file named \"kenya_monthly_summaries.geojson\" inst/ directory package, containing regional polygons associated monthly fishery metrics.#'","code":""},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/reference/create_geos.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Generate Geographic Regional Summaries of Fishery Data — create_geos","text":"function performs following operations: BMU Coordinate Extraction: Retrieves geographic coordinates (latitude/longitude) BMUs. Spatial Conversion: Converts BMU coordinates spatial point objects. Regional Assignment: Uses spatial analysis assign BMU nearest coastal region. Regional Aggregation: Calculates monthly summary statistics region aggregating BMU data. GeoJSON Creation: Combines regional polygon geometries summary statistics exports GeoJSON. Calculated Regional Metrics (using median values across BMUs region): Mean effort (fishers per square kilometer) Mean CPUE (Catch Per Unit Effort, kg per fisher) Mean CPUA (Catch Per Unit Area, kg per square kilometer) Mean RPUE (Revenue Per Unit Effort, currency per fisher) Mean RPUA (Revenue Per Unit Area, currency per square kilometer)","code":""},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/reference/create_geos.html","id":"note","dir":"Reference","previous_headings":"","what":"Note","title":"Generate Geographic Regional Summaries of Fishery Data — create_geos","text":"Dependencies: Requires sf package spatial operations. Requires GeoJSON file named \"KEN_coast_regions.geojson\" included peskas.kenya.data.pipeline package. Uses get_metadata() function retrieve BMU location information.","code":""},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/reference/create_geos.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Generate Geographic Regional Summaries of Fishery Data — create_geos","text":"","code":"if (FALSE) { # \\dontrun{ # First generate monthly summaries monthly_data <- get_fishery_metrics(validated_data, bmu_size)  # Then create regional geospatial summary create_geos(monthly_summaries_dat = monthly_data) } # }"},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/reference/df_to_airtable.html","id":null,"dir":"Reference","previous_headings":"","what":"Create New Airtable Records — df_to_airtable","title":"Create New Airtable Records — df_to_airtable","text":"Creates new records batches 10.","code":""},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/reference/df_to_airtable.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create New Airtable Records — df_to_airtable","text":"","code":"df_to_airtable(df, base_id, table_name, token)"},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/reference/df_to_airtable.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create New Airtable Records — df_to_airtable","text":"df Data frame containing data create. base_id Character string. Airtable base ID. table_name Character string. Name table. token Character string. Airtable API token.","code":""},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/reference/df_to_airtable.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Create New Airtable Records — df_to_airtable","text":"List response objects.","code":""},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/reference/download_cloud_file.html","id":null,"dir":"Reference","previous_headings":"","what":"Download Object from Cloud Storage — download_cloud_file","title":"Download Object from Cloud Storage — download_cloud_file","text":"Downloads object cloud storage local file.","code":""},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/reference/download_cloud_file.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Download Object from Cloud Storage — download_cloud_file","text":"","code":"download_cloud_file(name, provider, options, file = name)"},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/reference/download_cloud_file.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Download Object from Cloud Storage — download_cloud_file","text":"name name object storage bucket. provider character string specifying cloud provider (\"gcs\" \"aws\"). options named list provider-specific options including bucket authentication details. file (Optional) local path save downloaded object. specified, object name used.","code":""},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/reference/download_cloud_file.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Download Object from Cloud Storage — download_cloud_file","text":"path downloaded file.","code":""},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/reference/download_cloud_file.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Download Object from Cloud Storage — download_cloud_file","text":"GCS, options list include: bucket: name bucket object downloaded. service_account_key: authentication JSON contents, previously authenticated.","code":""},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/reference/download_cloud_file.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Download Object from Cloud Storage — download_cloud_file","text":"","code":"if (FALSE) { # \\dontrun{ authentication_details <- readLines(\"path/to/json_file.json\") download_cloud_file(   \"object_name.json\",   \"gcs\",   list(service_account_key = authentication_details, bucket = \"my-bucket\"),   \"local_path/to/save/object.json\" ) } # }"},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/reference/download_parquet_from_cloud.html","id":null,"dir":"Reference","previous_headings":"","what":"Download Parquet File from Cloud Storage — download_parquet_from_cloud","title":"Download Parquet File from Cloud Storage — download_parquet_from_cloud","text":"function handles process downloading parquet file cloud storage reading memory.","code":""},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/reference/download_parquet_from_cloud.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Download Parquet File from Cloud Storage — download_parquet_from_cloud","text":"","code":"download_parquet_from_cloud(prefix, provider, options)"},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/reference/download_parquet_from_cloud.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Download Parquet File from Cloud Storage — download_parquet_from_cloud","text":"prefix file prefix path cloud storage provider cloud storage provider key options Cloud storage provider options","code":""},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/reference/download_parquet_from_cloud.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Download Parquet File from Cloud Storage — download_parquet_from_cloud","text":"tibble containing data parquet file","code":""},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/reference/download_parquet_from_cloud.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Download Parquet File from Cloud Storage — download_parquet_from_cloud","text":"","code":"if (FALSE) { # \\dontrun{ raw_data <- download_parquet_from_cloud(   prefix = conf$ingestion$koboform$catch$legacy$raw,   provider = conf$storage$google$key,   options = conf$storage$google$options ) } # }"},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/reference/export_summaries.html","id":null,"dir":"Reference","previous_headings":"","what":"Export Summarized Fishery Data for Dashboard Integration — export_summaries","title":"Export Summarized Fishery Data for Dashboard Integration — export_summaries","text":"function processes exports validated fishery data calculating various summary metrics distributions, uploaded MongoDB collections usage dashboard.","code":""},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/reference/export_summaries.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Export Summarized Fishery Data for Dashboard Integration — export_summaries","text":"","code":"export_summaries(log_threshold = logger::DEBUG)"},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/reference/export_summaries.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Export Summarized Fishery Data for Dashboard Integration — export_summaries","text":"log_threshold logging threshold level monitoring operations (default: logger::DEBUG).","code":""},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/reference/export_summaries.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Export Summarized Fishery Data for Dashboard Integration — export_summaries","text":"function return value. uploads following collections MongoDB: Monthly catch summaries (monthly_stats monthly_summaries) Gear distribution statistics (gear_distribution) Fish distribution statistics (fish_distribution) Landing site mapping data (map_distribution)","code":""},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/reference/export_summaries.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Export Summarized Fishery Data for Dashboard Integration — export_summaries","text":"function performs following operations: Data Retrieval: Pulls validated fishery data \"legacy-validated\" MongoDB collection. Summary Dataset Generation: Creates following summary datasets: Monthly Statistics: Aggregates metrics like catch, effort, CPUE BMU (Beach Management Unit) month. Gear Distribution: Calculates percentage usage gear type landing site. Fish Distribution: Calculates percentage fish category landing site. Mapping Distribution: Prepares dataset landing sites geographic coordinates spatial mapping. Data Upload: Uploads summary datasets designated MongoDB collection. Calculated Metrics: Effort = Number fishers / Size BMU km² CPUE = Total catch kg / Effort Monthly Aggregations: Total catch (kg) Mean catch per trip Mean effort Mean CPUE (Catch Per Unit Effort) Gear Distribution: Count gear type used Percentage distribution gear types landing site Fish Distribution: Total catch fish category Percentage fish category within total catch","code":""},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/reference/export_summaries.html","id":"note","dir":"Reference","previous_headings":"","what":"Note","title":"Export Summarized Fishery Data for Dashboard Integration — export_summaries","text":"Dependencies: Requires configuration file compatible read_config function, containing MongoDB connection information. Access bmu_size dataset, provides size details BMUs, retrieved via get_metadata() function.","code":""},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/reference/export_summaries.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Export Summarized Fishery Data for Dashboard Integration — export_summaries","text":"","code":"if (FALSE) { # \\dontrun{ export_summaries() } # }"},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/reference/extract_trip_ids_from_filenames.html","id":null,"dir":"Reference","previous_headings":"","what":"Extract Trip IDs from Track Filenames — extract_trip_ids_from_filenames","title":"Extract Trip IDs from Track Filenames — extract_trip_ids_from_filenames","text":"Extract Trip IDs Track Filenames","code":""},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/reference/extract_trip_ids_from_filenames.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Extract Trip IDs from Track Filenames — extract_trip_ids_from_filenames","text":"","code":"extract_trip_ids_from_filenames(filenames)"},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/reference/extract_trip_ids_from_filenames.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Extract Trip IDs from Track Filenames — extract_trip_ids_from_filenames","text":"filenames Character vector track filenames","code":""},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/reference/extract_trip_ids_from_filenames.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Extract Trip IDs from Track Filenames — extract_trip_ids_from_filenames","text":"Character vector trip IDs","code":""},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/reference/flatten_field.html","id":null,"dir":"Reference","previous_headings":"","what":"Flatten a Single Field of Kobotoolbox Data — flatten_field","title":"Flatten a Single Field of Kobotoolbox Data — flatten_field","text":"internal function flattens single field within row Kobotoolbox data.","code":""},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/reference/flatten_field.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Flatten a Single Field of Kobotoolbox Data — flatten_field","text":"","code":"flatten_field(x, p)"},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/reference/flatten_field.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Flatten a Single Field of Kobotoolbox Data — flatten_field","text":"x field flattened. p name parent field.","code":""},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/reference/flatten_field.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Flatten a Single Field of Kobotoolbox Data — flatten_field","text":"flattened list representing input field.","code":""},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/reference/flatten_row.html","id":null,"dir":"Reference","previous_headings":"","what":"Flatten a Single Row of Kobotoolbox Data — flatten_row","title":"Flatten a Single Row of Kobotoolbox Data — flatten_row","text":"internal function flattens single row Kobotoolbox data, converting nested structures flat tibble.","code":""},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/reference/flatten_row.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Flatten a Single Row of Kobotoolbox Data — flatten_row","text":"","code":"flatten_row(x)"},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/reference/flatten_row.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Flatten a Single Row of Kobotoolbox Data — flatten_row","text":"x list representing single row Kobotoolbox data.","code":""},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/reference/flatten_row.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Flatten a Single Row of Kobotoolbox Data — flatten_row","text":"flattened tibble representing input row.","code":""},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/reference/generate_track_summaries.html","id":null,"dir":"Reference","previous_headings":"","what":"Generate Grid Summaries for Track Data — generate_track_summaries","title":"Generate Grid Summaries for Track Data — generate_track_summaries","text":"Processes GPS track data 1km grid summaries visualization analysis.","code":""},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/reference/generate_track_summaries.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Generate Grid Summaries for Track Data — generate_track_summaries","text":"","code":"generate_track_summaries(data, min_hours = 0.15, max_hours = 15)"},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/reference/generate_track_summaries.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Generate Grid Summaries for Track Data — generate_track_summaries","text":"data Preprocessed track data min_hours Minimum hours threshold filtering (default: 0.15) max_hours Maximum hours threshold filtering (default: 10)","code":""},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/reference/generate_track_summaries.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Generate Grid Summaries for Track Data — generate_track_summaries","text":"dataframe grid summary statistics","code":""},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/reference/get_catch_bounds.html","id":null,"dir":"Reference","previous_headings":"","what":"Get fish groups Catch Bounds — get_catch_bounds","title":"Get fish groups Catch Bounds — get_catch_bounds","text":"Calculates upper bounds fish groups catch data (using catch_kg) based gear type fish category. Data grouped interaction gear fish category, category \"0\" excluded analysis.","code":""},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/reference/get_catch_bounds.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get fish groups Catch Bounds — get_catch_bounds","text":"","code":"get_catch_bounds(data = NULL, k = NULL)"},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/reference/get_catch_bounds.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get fish groups Catch Bounds — get_catch_bounds","text":"data data frame containing columns: gear, fish_category, catch_kg. k numeric value used univOutl::LocScaleB function outlier detection.","code":""},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/reference/get_catch_bounds.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Get fish groups Catch Bounds — get_catch_bounds","text":"data frame columns: gear, fish_category, upper.(upper bound).","code":""},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/reference/get_catch_bounds_iqr.html","id":null,"dir":"Reference","previous_headings":"","what":"Get fish groups Catch Bounds using IQR method — get_catch_bounds_iqr","title":"Get fish groups Catch Bounds using IQR method — get_catch_bounds_iqr","text":"Get fish groups Catch Bounds using IQR method","code":""},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/reference/get_catch_bounds_iqr.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get fish groups Catch Bounds using IQR method — get_catch_bounds_iqr","text":"","code":"get_catch_bounds_iqr(data = NULL, multiplier = 1.5)"},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/reference/get_catch_bounds_iqr.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get fish groups Catch Bounds using IQR method — get_catch_bounds_iqr","text":"data data frame containing columns: gear, fish_category, catch_kg multiplier multiplier IQR range (default 1.5)","code":""},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/reference/get_catch_bounds_iqr.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Get fish groups Catch Bounds using IQR method — get_catch_bounds_iqr","text":"data frame columns: gear, fish_category, upper.","code":""},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/reference/get_fishery_metrics.html","id":null,"dir":"Reference","previous_headings":"","what":"Calculate Fishery Performance Metrics — get_fishery_metrics","title":"Calculate Fishery Performance Metrics — get_fishery_metrics","text":"Calculates key fishery performance metrics validated catch data, including effort, CPUE (Catch Per Unit Effort), CPUA (Catch Per Unit Area), RPUE (Revenue Per Unit Effort), RPUA (Revenue Per Unit Area) Price per kg.","code":""},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/reference/get_fishery_metrics.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Calculate Fishery Performance Metrics — get_fishery_metrics","text":"","code":"get_fishery_metrics(valid_data = NULL, bmu_size = NULL)"},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/reference/get_fishery_metrics.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Calculate Fishery Performance Metrics — get_fishery_metrics","text":"valid_data data frame containing validated fishery data columns: landing_site: Name landing site (renamed BMU) landing_date: Date landing no_of_fishers: Number fishers total_catch_kg: Total catch kilograms total_catch_price: Total catch value currency bmu_size data frame containing BMU size information columns: BMU: Name Beach Management Unit (lowercase) size_km: Size BMU square kilometers","code":""},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/reference/get_fishery_metrics.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Calculate Fishery Performance Metrics — get_fishery_metrics","text":"data frame monthly aggregated metrics BMU: BMU: Name Beach Management Unit (title case) date: First day month mean_effort: Average fishers per square kilometer mean_cpue: Average catch (kg) per fisher mean_cpua: Average catch (kg) per square kilometer mean_rpue: Average revenue per fisher mean_rpua: Average revenue per square kilometer mean_price_kg: Average price per kg","code":""},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/reference/get_fishery_metrics.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Calculate Fishery Performance Metrics — get_fishery_metrics","text":"Metrics calculated follows: Effort = Total fishers / BMU size (km²) CPUE = Total catch (kg) / Total fishers CPUA = Total catch (kg) / BMU size (km²) RPUE = Total revenue / Total fishers RPUA = Total revenue / BMU size (km²) Price per kg = Total revenue / Total catch (single submissions level)","code":""},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/reference/get_fishery_metrics_long.html","id":null,"dir":"Reference","previous_headings":"","what":"Calculate key fishery metrics by landing site and month in normalized long format — get_fishery_metrics_long","title":"Calculate key fishery metrics by landing site and month in normalized long format — get_fishery_metrics_long","text":"Summarizes fishery data extract main characteristics including catch rates, gear usage, species composition, CPUE RPUE gear type. Returns data fully normalized long format maximum interoperability analytical flexibility.","code":""},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/reference/get_fishery_metrics_long.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Calculate key fishery metrics by landing site and month in normalized long format — get_fishery_metrics_long","text":"","code":"get_fishery_metrics_long(data = NULL)"},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/reference/get_fishery_metrics_long.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Calculate key fishery metrics by landing site and month in normalized long format — get_fishery_metrics_long","text":"data dataframe containing fishery landing data columns: submission_id, landing_date, landing_site, gear, no_of_fishers, fish_category, catch_kg, total_catch_price","code":""},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/reference/get_fishery_metrics_long.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Calculate key fishery metrics by landing site and month in normalized long format — get_fishery_metrics_long","text":"dataframe long format columns: landing_site: Name landing site year_month: First day month metric_type: Type metric (e.g., \"avg_fishers_per_trip\", \"cpue\", \"rpue\", \"species_pct\") metric_value: Numeric value metric gear_type: Type fishing gear (gear-specific metrics, NA site-level metrics) species: Fish species name (species-specific metrics, NA metrics) rank: Rank order (ranked metrics like top species, NA others)","code":""},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/reference/get_fishery_metrics_long.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Calculate key fishery metrics by landing site and month in normalized long format — get_fishery_metrics_long","text":"function creates fully normalized dataset row represents single metric observation. format enables: Easy filtering metric type, gear, species Flexible aggregation comparison across dimensions Database-friendly structure storage querying Simplified visualization statistical analysis","code":""},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/reference/get_fishery_metrics_long.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Calculate key fishery metrics by landing site and month in normalized long format — get_fishery_metrics_long","text":"","code":"if (FALSE) { # \\dontrun{ fishery_metrics <- get_fishery_metrics_long(data = valid_data)  # Filter for CPUE metrics only cpue_data <- fishery_metrics %>% filter(metric_type == \"cpue\")  # Filter for RPUE metrics only rpue_data <- fishery_metrics %>% filter(metric_type == \"rpue\")  # Get predominant gear by site main_gear <- fishery_metrics %>% filter(metric_type == \"predominant_gear\") } # }"},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/reference/get_individual_data.html","id":null,"dir":"Reference","previous_headings":"","what":"Get fishers ID and trip cost — get_individual_data","title":"Get fishers ID and trip cost — get_individual_data","text":"function extracts fisher ID trip cost raw data.","code":""},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/reference/get_individual_data.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get fishers ID and trip cost — get_individual_data","text":"","code":"get_individual_data(raw_dat)"},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/reference/get_individual_data.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get fishers ID and trip cost — get_individual_data","text":"raw_dat Raw data cloud storage","code":""},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/reference/get_individual_data.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Get fishers ID and trip cost — get_individual_data","text":"data frame submission ID, fisher ID, trip cost","code":""},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/reference/get_individual_gear_metrics.html","id":null,"dir":"Reference","previous_headings":"","what":"Calculate Individual Fisher Performance Metrics by Gear Type — get_individual_gear_metrics","title":"Calculate Individual Fisher Performance Metrics by Gear Type — get_individual_gear_metrics","text":"Calculates key fishery performance metrics individual fisher level validated catch data, stratified gear type. function provides metrics get_individual_metrics includes gear type additional grouping variable, allowing analysis gear-specific performance patterns.","code":""},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/reference/get_individual_gear_metrics.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Calculate Individual Fisher Performance Metrics by Gear Type — get_individual_gear_metrics","text":"","code":"get_individual_gear_metrics(valid_data = NULL)"},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/reference/get_individual_gear_metrics.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Calculate Individual Fisher Performance Metrics by Gear Type — get_individual_gear_metrics","text":"valid_data data frame containing validated fishery data columns: landing_site: Name landing site (renamed BMU) landing_date: Date landing fisher_id: Unique identifier fisher gear: Type fishing gear used no_of_fishers: Number fishers total_catch_kg: Total catch kilograms total_catch_price: Total catch value currency trip_cost: Cost fishing trip currency","code":""},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/reference/get_individual_gear_metrics.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Calculate Individual Fisher Performance Metrics by Gear Type — get_individual_gear_metrics","text":"data frame monthly aggregated metrics BMU, gear type, individual fisher: BMU: Name Beach Management Unit (title case) date: First day month fisher_id: Unique identifier fisher gear: Type fishing gear used mean_cpue: Average catch (kg) per fisher mean_rpue: Average revenue per fisher mean_price_kg: Average price per kg mean_cost: Average trip costs per fisher mean_profit: Average profit per fisher (revenue minus costs)","code":""},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/reference/get_individual_gear_metrics.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Calculate Individual Fisher Performance Metrics by Gear Type — get_individual_gear_metrics","text":"function extends individual fisher analysis incorporating gear type additional stratification variable. Individual fisher metrics calculated follows: CPUE = Aggregated catch (kg) / Total fishers RPUE = Aggregated revenue / Total fishers Price per kg = Total revenue / Total catch (median across trips) Costs = Median trip cost Profit = RPUE - Costs function filters records non-missing landing dates fisher IDs, aggregates data landing date, BMU, gear type, fisher ID calculating monthly averages. allows comparison performance across different gear types fisher, well gear-specific benchmarking across fishers. fishers individual identification included analysis.","code":""},{"path":[]},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/reference/get_individual_metrics.html","id":null,"dir":"Reference","previous_headings":"","what":"Calculate Individual Fisher Performance Metrics — get_individual_metrics","title":"Calculate Individual Fisher Performance Metrics — get_individual_metrics","text":"Calculates key fishery performance metrics individual fisher level validated catch data, including CPUE (Catch Per Unit Effort), RPUE (Revenue Per Unit Effort), price per kg, trip costs, profit margins fisher.","code":""},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/reference/get_individual_metrics.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Calculate Individual Fisher Performance Metrics — get_individual_metrics","text":"","code":"get_individual_metrics(valid_data = NULL)"},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/reference/get_individual_metrics.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Calculate Individual Fisher Performance Metrics — get_individual_metrics","text":"valid_data data frame containing validated fishery data columns: landing_site: Name landing site (renamed BMU) landing_date: Date landing fisher_id: Unique identifier fisher no_of_fishers: Number fishers total_catch_kg: Total catch kilograms total_catch_price: Total catch value currency trip_cost: Cost fishing trip currency","code":""},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/reference/get_individual_metrics.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Calculate Individual Fisher Performance Metrics — get_individual_metrics","text":"data frame monthly aggregated metrics BMU individual fisher: BMU: Name Beach Management Unit (title case) date: First day month fisher_id: Unique identifier fisher mean_cpue: Average catch (kg) per fisher mean_rpue: Average revenue per fisher mean_price_kg: Average price per kg mean_cost: Average trip costs per fisher mean_profit: Average profit per fisher (revenue minus costs)","code":""},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/reference/get_individual_metrics.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Calculate Individual Fisher Performance Metrics — get_individual_metrics","text":"Individual fisher metrics calculated follows: CPUE = Aggregated catch (kg) / Total fishers RPUE = Aggregated revenue / Total fishers Price per kg = Total revenue / Total catch (median across trips) Costs = Median trip cost Profit = RPUE - Costs function filters records non-missing landing dates fisher IDs, aggregates data landing date, BMU, fisher ID calculating monthly averages. fishers individual identification included analysis.","code":""},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/reference/get_kobo_data.html","id":null,"dir":"Reference","previous_headings":"","what":"Retrieve Data from Kobotoolbox API — get_kobo_data","title":"Retrieve Data from Kobotoolbox API — get_kobo_data","text":"function retrieves survey data Kobotoolbox API specific asset. supports pagination handles JSON XML formats.","code":""},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/reference/get_kobo_data.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Retrieve Data from Kobotoolbox API — get_kobo_data","text":"","code":"get_kobo_data(   assetid,   url = \"eu.kobotoolbox.org\",   uname = NULL,   pwd = NULL,   encoding = \"UTF-8\",   format = \"json\" )"},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/reference/get_kobo_data.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Retrieve Data from Kobotoolbox API — get_kobo_data","text":"assetid asset ID Kobotoolbox form. url URL Kobotoolbox (default \"eu.kobotoolbox.org\"). uname Username Kobotoolbox account. pwd Password Kobotoolbox account. encoding Encoding used data retrieval (default \"UTF-8\"). format Format data retrieve, either \"json\" \"xml\" (default \"json\").","code":""},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/reference/get_kobo_data.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Retrieve Data from Kobotoolbox API — get_kobo_data","text":"list containing retrieved survey results.","code":""},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/reference/get_kobo_data.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Retrieve Data from Kobotoolbox API — get_kobo_data","text":"function uses pagination retrieve large datasets, limit 30,000 records per request. continues fetch data records retrieved error occurs.","code":""},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/reference/get_kobo_data.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Retrieve Data from Kobotoolbox API — get_kobo_data","text":"","code":"if (FALSE) { # \\dontrun{ kobo_data <- get_kobo_data(   assetid = \"your_asset_id\",   uname = \"your_username\",   pwd = \"your_password\" ) } # }"},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/reference/get_metadata.html","id":null,"dir":"Reference","previous_headings":"","what":"Get metadata tables — get_metadata","title":"Get metadata tables — get_metadata","text":"Get Metadata tables Google sheets. function downloads tables include information fishery.","code":""},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/reference/get_metadata.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get metadata tables — get_metadata","text":"","code":"get_metadata(log_threshold = logger::DEBUG)"},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/reference/get_metadata.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get metadata tables — get_metadata","text":"log_threshold logging threshold level. Default logger::DEBUG.","code":""},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/reference/get_metadata.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Get metadata tables — get_metadata","text":"parameters needed conf.yml :","code":"storage:   storage_name:     key:     options:       project:       bucket:       service_account_key:"},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/reference/get_metadata.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Get metadata tables — get_metadata","text":"","code":"if (FALSE) { # \\dontrun{ # Ensure you have the necessary configuration in conf.yml metadata_tables <- get_metadata() } # }"},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/reference/get_total_catch_bounds.html","id":null,"dir":"Reference","previous_headings":"","what":"Get Total Catch Bounds — get_total_catch_bounds","title":"Get Total Catch Bounds — get_total_catch_bounds","text":"Calculates upper bounds total catch data (using total_catch_kg) based landing site gear type combinations. NA values total_catch_kg filtered analysis. function groups data combined landing_site gear identifiers calculating bounds.","code":""},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/reference/get_total_catch_bounds.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get Total Catch Bounds — get_total_catch_bounds","text":"","code":"get_total_catch_bounds(data = NULL, k = NULL)"},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/reference/get_total_catch_bounds.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get Total Catch Bounds — get_total_catch_bounds","text":"data data frame containing columns: gear, landing_site, submission_id total_catch_kg. k numeric value used univOutl::LocScaleB function outlier detection.","code":""},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/reference/get_total_catch_bounds.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Get Total Catch Bounds — get_total_catch_bounds","text":"data frame columns: landing_site, gear upper.(upper bound).","code":""},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/reference/get_total_catch_bounds_iqr.html","id":null,"dir":"Reference","previous_headings":"","what":"Get Total Catch Bounds using IQR method — get_total_catch_bounds_iqr","title":"Get Total Catch Bounds using IQR method — get_total_catch_bounds_iqr","text":"Get Total Catch Bounds using IQR method","code":""},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/reference/get_total_catch_bounds_iqr.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get Total Catch Bounds using IQR method — get_total_catch_bounds_iqr","text":"","code":"get_total_catch_bounds_iqr(data = NULL, multiplier = 1.5)"},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/reference/get_total_catch_bounds_iqr.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get Total Catch Bounds using IQR method — get_total_catch_bounds_iqr","text":"data data frame containing required columns multiplier multiplier IQR range (default 1.5)","code":""},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/reference/get_total_catch_bounds_iqr.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Get Total Catch Bounds using IQR method — get_total_catch_bounds_iqr","text":"data frame upper bounds landing site gear combination","code":""},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/reference/get_trip_points.html","id":null,"dir":"Reference","previous_headings":"","what":"Get Trip Points from Pelagic Data Systems API — get_trip_points","title":"Get Trip Points from Pelagic Data Systems API — get_trip_points","text":"Retrieves trip points data Pelagic Data Systems API. function can either fetch data specific trip ID date range. response can returned data frame written directly file.","code":""},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/reference/get_trip_points.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get Trip Points from Pelagic Data Systems API — get_trip_points","text":"","code":"get_trip_points(   token = NULL,   secret = NULL,   id = NULL,   dateFrom = NULL,   dateTo = NULL,   path = NULL,   imeis = NULL,   deviceInfo = FALSE,   errant = FALSE,   withLastSeen = FALSE,   tags = NULL,   overwrite = TRUE )"},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/reference/get_trip_points.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get Trip Points from Pelagic Data Systems API — get_trip_points","text":"token Character string. Access token PDS API. secret Character string. Secret key PDS API. id Numeric character. Optional trip ID. provided, retrieves points specific trip. NULL, dateFrom dateTo must provided. dateFrom Character string. Start date data retrieval format \"YYYY-MM-DD\". Required id NULL. dateTo Character string. End date data retrieval format \"YYYY-MM-DD\". Required id NULL. path Character string. Optional path CSV file saved. provided, function returns path instead data frame. imeis Vector character numeric. Optional IMEI numbers filter data. deviceInfo Logical. TRUE, includes device information response. Default FALSE. errant Logical. TRUE, includes errant points response. Default FALSE. withLastSeen Logical. TRUE, includes last seen information. Default FALSE. tags Vector character. Optional tags filter data. overwrite Logical. TRUE, overwrite existing file path provided. Default TRUE.","code":""},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/reference/get_trip_points.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Get Trip Points from Pelagic Data Systems API — get_trip_points","text":"path NULL, returns tibble containing trip points data. path provided, returns file path character string.","code":""},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/reference/get_trip_points.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Get Trip Points from Pelagic Data Systems API — get_trip_points","text":"","code":"if (FALSE) { # \\dontrun{ # Get data for a specific trip trip_data <- get_trip_points(   token = \"your_token\",   secret = \"your_secret\",   id = \"12345\",   deviceInfo = TRUE )  # Get data for a date range date_data <- get_trip_points(   token = \"your_token\",   secret = \"your_secret\",   dateFrom = \"2024-01-01\",   dateTo = \"2024-01-31\" )  # Save data directly to file file_path <- get_trip_points(   token = \"your_token\",   secret = \"your_secret\",   id = \"12345\",   path = \"trip_data.csv\" ) } # }"},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/reference/get_trips.html","id":null,"dir":"Reference","previous_headings":"","what":"Retrieve Trip Details from Pelagic Data API — get_trips","title":"Retrieve Trip Details from Pelagic Data API — get_trips","text":"function retrieves trip details Pelagic Data API specified time range, options filter IMEIs include additional information.","code":""},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/reference/get_trips.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Retrieve Trip Details from Pelagic Data API — get_trips","text":"","code":"get_trips(   token = NULL,   secret = NULL,   dateFrom = NULL,   dateTo = NULL,   imeis = NULL,   deviceInfo = FALSE,   withLastSeen = FALSE,   tags = NULL )"},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/reference/get_trips.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Retrieve Trip Details from Pelagic Data API — get_trips","text":"token Character string. API token authentication. secret Character string. API secret authentication. dateFrom Character string. Start date 'YYYY-MM-dd' format. dateTo Character string. End date 'YYYY-MM-dd' format. imeis Character vector. Optional. Filter IMEI numbers. deviceInfo Logical. TRUE, include device IMEI ID fields response. Default FALSE. withLastSeen Logical. TRUE, include device last seen date response. Default FALSE. tags Character vector. Optional. Filter trip tags.","code":""},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/reference/get_trips.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Retrieve Trip Details from Pelagic Data API — get_trips","text":"data frame containing trip details.","code":""},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/reference/get_trips.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Retrieve Trip Details from Pelagic Data API — get_trips","text":"","code":"if (FALSE) { # \\dontrun{ trips <- get_trips(   token = \"your_token\",   secret = \"your_secret\",   dateFrom = \"2020-05-01\",   dateTo = \"2020-05-03\",   imeis = c(\"123456789\", \"987654321\"),   deviceInfo = TRUE,   withLastSeen = TRUE,   tags = c(\"tag1\", \"tag2\") ) } # }"},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/reference/get_writable_fields.html","id":null,"dir":"Reference","previous_headings":"","what":"Get Writable Fields from Airtable Table — get_writable_fields","title":"Get Writable Fields from Airtable Table — get_writable_fields","text":"Returns fields can updated (excludes computed fields).","code":""},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/reference/get_writable_fields.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get Writable Fields from Airtable Table — get_writable_fields","text":"","code":"get_writable_fields(base_id, token, table_name)"},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/reference/get_writable_fields.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get Writable Fields from Airtable Table — get_writable_fields","text":"base_id Character string. Airtable base ID. token Character string. Airtable API token. table_name Character string. Name table.","code":""},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/reference/get_writable_fields.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Get Writable Fields from Airtable Table — get_writable_fields","text":"Character vector writable field names.","code":""},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/reference/impute_price.html","id":null,"dir":"Reference","previous_headings":"","what":"Impute Missing Fish Prices Using Median Values — impute_price","title":"Impute Missing Fish Prices Using Median Values — impute_price","text":"function imputes missing fish prices two steps: fish size (small/large): uses median price landing sites fish NA size: uses median small large sizes","code":""},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/reference/impute_price.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Impute Missing Fish Prices Using Median Values — impute_price","text":"","code":"impute_price(price_table = NULL)"},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/reference/impute_price.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Impute Missing Fish Prices Using Median Values — impute_price","text":"price_table tibble containing fish price data columns: date: Date record landing_site: Name landing site fish_category: Type fish size: Size category fish (large, small, NA) median_ksh_kg: Original price Kenyan Shillings per kg","code":""},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/reference/impute_price.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Impute Missing Fish Prices Using Median Values — impute_price","text":"tibble structure input, : possible combinations date, landing_site, fish_category valid sizes Original median_ksh_kg column removed New median_ksh_kg_imputed column containing original imputed prices","code":""},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/reference/impute_price.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Impute Missing Fish Prices Using Median Values — impute_price","text":"","code":"if (FALSE) { # \\dontrun{ imputed_data <- impute_price(price_table = fish_prices) } # }"},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/reference/ingest_catch_survey_version.html","id":null,"dir":"Reference","previous_headings":"","what":"Core ingestion logic for catch survey data — ingest_catch_survey_version","title":"Core ingestion logic for catch survey data — ingest_catch_survey_version","text":"Core ingestion logic catch survey data","code":""},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/reference/ingest_catch_survey_version.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Core ingestion logic for catch survey data — ingest_catch_survey_version","text":"","code":"ingest_catch_survey_version(version, kobo_config, storage_config)"},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/reference/ingest_catch_survey_version.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Core ingestion logic for catch survey data — ingest_catch_survey_version","text":"version Version identifier (e.g., \"v1\", \"v2\") kobo_config Configuration object containing Kobo connection details storage_config Configuration object containing storage details","code":""},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/reference/ingest_catch_survey_version.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Core ingestion logic for catch survey data — ingest_catch_survey_version","text":"return value. Processes uploads data.","code":""},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/reference/ingest_kefs_surveys.html","id":null,"dir":"Reference","previous_headings":"","what":"Download and Process KEFS Catch Surveys from Kobotoolbox — ingest_kefs_surveys","title":"Download and Process KEFS Catch Surveys from Kobotoolbox — ingest_kefs_surveys","text":"function retrieves KEFS (Kenya Fisheries Service) survey data dedicated Kobo instance, processes , uploads raw data Parquet file Google Cloud Storage.","code":""},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/reference/ingest_kefs_surveys.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Download and Process KEFS Catch Surveys from Kobotoolbox — ingest_kefs_surveys","text":"","code":"ingest_kefs_surveys(log_threshold = logger::DEBUG)"},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/reference/ingest_kefs_surveys.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Download and Process KEFS Catch Surveys from Kobotoolbox — ingest_kefs_surveys","text":"log_threshold Logging threshold level (default: logger::DEBUG)","code":""},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/reference/ingest_kefs_surveys.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Download and Process KEFS Catch Surveys from Kobotoolbox — ingest_kefs_surveys","text":"return value. Function downloads data, processes , uploads Google Cloud Storage.","code":""},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/reference/ingest_kefs_surveys.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Download and Process KEFS Catch Surveys from Kobotoolbox — ingest_kefs_surveys","text":"function performs following steps: Reads configuration settings. Downloads survey data KEFS Kobo instance (kf.fimskenya.co.ke). Checks uniqueness submissions. Converts data tabular format. Uploads raw data Parquet file Google Cloud Storage. Note: Preprocessing, validation, export stages KEFS data yet implemented.","code":""},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/reference/ingest_kefs_surveys.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Download and Process KEFS Catch Surveys from Kobotoolbox — ingest_kefs_surveys","text":"","code":"if (FALSE) { # \\dontrun{ ingest_kefs_surveys() } # }"},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/reference/ingest_landings.html","id":null,"dir":"Reference","previous_headings":"","what":"Download and Process All Catch Surveys (WCS + KEFS) — ingest_landings","title":"Download and Process All Catch Surveys (WCS + KEFS) — ingest_landings","text":"DEPRECATED: function maintained backward compatibility removed future version. Use ingest_wcs_surveys() ingest_kefs_surveys() separately instead. function ingests WCS KEFS catch surveys single call, creates coupling data sources. one source fails, fail.","code":""},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/reference/ingest_landings.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Download and Process All Catch Surveys (WCS + KEFS) — ingest_landings","text":"","code":"ingest_landings(log_threshold = logger::DEBUG)"},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/reference/ingest_landings.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Download and Process All Catch Surveys (WCS + KEFS) — ingest_landings","text":"log_threshold Logging threshold level (default: logger::DEBUG)","code":""},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/reference/ingest_landings.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Download and Process All Catch Surveys (WCS + KEFS) — ingest_landings","text":"return value. Function downloads data, processes , uploads Google Cloud Storage.","code":""},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/reference/ingest_landings.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Download and Process All Catch Surveys (WCS + KEFS) — ingest_landings","text":"","code":"if (FALSE) { # \\dontrun{ # Old way (deprecated): ingest_landings()  # New way (preferred): ingest_wcs_surveys() ingest_kefs_surveys() } # }"},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/reference/ingest_landings_price.html","id":null,"dir":"Reference","previous_headings":"","what":"Download and Process WCS Price Surveys from Kobotoolbox — ingest_landings_price","title":"Download and Process WCS Price Surveys from Kobotoolbox — ingest_landings_price","text":"function retrieves survey data Kobotoolbox specific project, processes , uploads raw data Parquet file Google Cloud Storage. uses get_kobo_data function, wrapper kobotools_kpi_data KoboconnectR package.","code":""},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/reference/ingest_landings_price.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Download and Process WCS Price Surveys from Kobotoolbox — ingest_landings_price","text":"","code":"ingest_landings_price(   versions = c(\"v1\", \"v2\"),   url = NULL,   project_id = NULL,   username = NULL,   psswd = NULL,   encoding = NULL )"},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/reference/ingest_landings_price.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Download and Process WCS Price Surveys from Kobotoolbox — ingest_landings_price","text":"versions Character vector versions process (default: c(\"v1\", \"v2\")) url URL Kobotoolbox (default NULL, uses value configuration). project_id asset ID project download data (default NULL, uses value configuration). username Username Kobotoolbox account (default NULL, uses value configuration). psswd Password Kobotoolbox account (default NULL, uses value configuration). encoding Encoding used data retrieval (default NULL, uses \"UTF-8\").","code":""},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/reference/ingest_landings_price.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Download and Process WCS Price Surveys from Kobotoolbox — ingest_landings_price","text":"return value. Function downloads data, processes , uploads Google Cloud Storage.","code":""},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/reference/ingest_landings_price.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Download and Process WCS Price Surveys from Kobotoolbox — ingest_landings_price","text":"function performs following steps: Reads configuration settings. Downloads survey data Kobotoolbox using get_kobo_data. Checks uniqueness submissions. Converts data tabular format. Uploads raw data Parquet file Google Cloud Storage. Note parameters provided customization, function currently uses hardcoded values configuration settings parameters.","code":""},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/reference/ingest_landings_price.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Download and Process WCS Price Surveys from Kobotoolbox — ingest_landings_price","text":"","code":"if (FALSE) { # \\dontrun{ # Process both versions ingest_landings_price()  # Process only v1 ingest_landings_price(versions = \"v1\")  # Process specific versions ingest_landings_price(versions = c(\"v1\", \"v2\")) } # }"},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/reference/ingest_pds_tracks.html","id":null,"dir":"Reference","previous_headings":"","what":"Ingest Pelagic Data Systems (PDS) Track Data — ingest_pds_tracks","title":"Ingest Pelagic Data Systems (PDS) Track Data — ingest_pds_tracks","text":"function handles automated ingestion GPS boat track data Pelagic Data Systems (PDS). downloads stores new tracks previously uploaded Google Cloud Storage. Uses parallel processing improved performance.","code":""},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/reference/ingest_pds_tracks.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Ingest Pelagic Data Systems (PDS) Track Data — ingest_pds_tracks","text":"","code":"ingest_pds_tracks(log_threshold = logger::DEBUG, batch_size = NULL)"},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/reference/ingest_pds_tracks.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Ingest Pelagic Data Systems (PDS) Track Data — ingest_pds_tracks","text":"log_threshold logging threshold use. Default logger::DEBUG. batch_size Optional number tracks process. NULL, processes new tracks.","code":""},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/reference/ingest_pds_tracks.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Ingest Pelagic Data Systems (PDS) Track Data — ingest_pds_tracks","text":"None (invisible). function performs operations side effects.","code":""},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/reference/ingest_pds_trips.html","id":null,"dir":"Reference","previous_headings":"","what":"Ingest Pelagic Data Systems (PDS) Trip Data — ingest_pds_trips","title":"Ingest Pelagic Data Systems (PDS) Trip Data — ingest_pds_trips","text":"function handles automated ingestion GPS boat trip data Pelagic Data Systems (PDS). performs following operations: Retrieves device metadata configured source Downloads trip data PDS API using device IMEIs Converts data parquet format Uploads processed file configured cloud storage","code":""},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/reference/ingest_pds_trips.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Ingest Pelagic Data Systems (PDS) Trip Data — ingest_pds_trips","text":"","code":"ingest_pds_trips(log_threshold = logger::DEBUG)"},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/reference/ingest_pds_trips.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Ingest Pelagic Data Systems (PDS) Trip Data — ingest_pds_trips","text":"log_threshold logging threshold use. Default logger::DEBUG. See logger::log_levels available options.","code":""},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/reference/ingest_pds_trips.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Ingest Pelagic Data Systems (PDS) Trip Data — ingest_pds_trips","text":"None (invisible). function performs operations side effects: Creates parquet file locally trip data Uploads file configured cloud storage Generates logs process","code":""},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/reference/ingest_pds_trips.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Ingest Pelagic Data Systems (PDS) Trip Data — ingest_pds_trips","text":"function requires specific configuration conf.yml file following structure:   function processes trips sequentially: Retrieves device metadata using get_metadata() Downloads trip data using get_trips() function Converts data parquet format Uploads resulting file configured storage provider","code":"pds:   token: \"your_pds_token\"               # PDS API token   secret: \"your_pds_secret\"             # PDS API secret   pds_trips:     file_prefix: \"pds_trips\"            # Prefix for output files storage:   google:                               # Storage provider name     key: \"google\"                       # Storage provider identifier     options:       project: \"project-id\"             # Cloud project ID       bucket: \"bucket-name\"             # Storage bucket name       service_account_key: \"path/to/key.json\""},{"path":[]},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/reference/ingest_pds_trips.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Ingest Pelagic Data Systems (PDS) Trip Data — ingest_pds_trips","text":"","code":"if (FALSE) { # \\dontrun{ # Run with default debug logging ingest_pds_trips()  # Run with info-level logging only ingest_pds_trips(logger::INFO) } # }"},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/reference/ingest_price_survey_version.html","id":null,"dir":"Reference","previous_headings":"","what":"Core ingestion logic for price survey data — ingest_price_survey_version","title":"Core ingestion logic for price survey data — ingest_price_survey_version","text":"Core ingestion logic price survey data","code":""},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/reference/ingest_price_survey_version.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Core ingestion logic for price survey data — ingest_price_survey_version","text":"","code":"ingest_price_survey_version(version, kobo_config, storage_config)"},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/reference/ingest_price_survey_version.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Core ingestion logic for price survey data — ingest_price_survey_version","text":"version Version identifier (e.g., \"v1\", \"v2\") kobo_config Configuration object containing Kobo connection details storage_config Configuration object containing storage details","code":""},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/reference/ingest_price_survey_version.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Core ingestion logic for price survey data — ingest_price_survey_version","text":"return value. Processes uploads data.","code":""},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/reference/ingest_wcs_surveys.html","id":null,"dir":"Reference","previous_headings":"","what":"Download and Process WCS Catch Surveys from Kobotoolbox — ingest_wcs_surveys","title":"Download and Process WCS Catch Surveys from Kobotoolbox — ingest_wcs_surveys","text":"function retrieves WCS survey data (v1 v2) Kobotoolbox, processes , uploads raw data Parquet files Google Cloud Storage.","code":""},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/reference/ingest_wcs_surveys.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Download and Process WCS Catch Surveys from Kobotoolbox — ingest_wcs_surveys","text":"","code":"ingest_wcs_surveys(log_threshold = logger::DEBUG)"},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/reference/ingest_wcs_surveys.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Download and Process WCS Catch Surveys from Kobotoolbox — ingest_wcs_surveys","text":"log_threshold Logging threshold level (default: logger::DEBUG)","code":""},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/reference/ingest_wcs_surveys.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Download and Process WCS Catch Surveys from Kobotoolbox — ingest_wcs_surveys","text":"return value. Function downloads data, processes , uploads Google Cloud Storage.","code":""},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/reference/ingest_wcs_surveys.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Download and Process WCS Catch Surveys from Kobotoolbox — ingest_wcs_surveys","text":"function performs following steps: Reads configuration settings. Downloads survey data Kobotoolbox using get_kobo_data. Checks uniqueness submissions. Converts data tabular format. Uploads raw data Parquet files Google Cloud Storage. function processes WCS v1 (eu.kobotoolbox.org) v2 (kf.kobotoolbox.org) catch surveys.","code":""},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/reference/ingest_wcs_surveys.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Download and Process WCS Catch Surveys from Kobotoolbox — ingest_wcs_surveys","text":"","code":"if (FALSE) { # \\dontrun{ ingest_wcs_surveys() } # }"},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/reference/mdb_collection_pull.html","id":null,"dir":"Reference","previous_headings":"","what":"Retrieve Data from MongoDB — mdb_collection_pull","title":"Retrieve Data from MongoDB — mdb_collection_pull","text":"function connects MongoDB database retrieves documents specified collection, maintaining original column order available.","code":""},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/reference/mdb_collection_pull.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Retrieve Data from MongoDB — mdb_collection_pull","text":"","code":"mdb_collection_pull(   connection_string = NULL,   collection_name = NULL,   db_name = NULL )"},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/reference/mdb_collection_pull.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Retrieve Data from MongoDB — mdb_collection_pull","text":"connection_string character string specifying MongoDB connection URL. Default NULL. collection_name character string specifying name collection query. Default NULL. db_name character string specifying name database. Default NULL.","code":""},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/reference/mdb_collection_pull.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Retrieve Data from MongoDB — mdb_collection_pull","text":"data frame containing documents specified collection, columns ordered data originally pushed MongoDB.","code":""},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/reference/mdb_collection_pull.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Retrieve Data from MongoDB — mdb_collection_pull","text":"","code":"if (FALSE) { # \\dontrun{ # Retrieve data from a MongoDB collection result <- mdb_collection_pull(   connection_string = \"mongodb://localhost:27017\",   collection_name = \"my_collection\",   db_name = \"my_database\" ) } # }"},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/reference/mdb_collection_push.html","id":null,"dir":"Reference","previous_headings":"","what":"Upload Data to MongoDB and Overwrite Existing Content — mdb_collection_push","title":"Upload Data to MongoDB and Overwrite Existing Content — mdb_collection_push","text":"function connects MongoDB database, removes existing documents specified collection, inserts new data. also stores original column order maintain data structure consistency.","code":""},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/reference/mdb_collection_push.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Upload Data to MongoDB and Overwrite Existing Content — mdb_collection_push","text":"","code":"mdb_collection_push(   data = NULL,   connection_string = NULL,   collection_name = NULL,   db_name = NULL )"},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/reference/mdb_collection_push.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Upload Data to MongoDB and Overwrite Existing Content — mdb_collection_push","text":"data data frame containing data uploaded. connection_string character string specifying MongoDB connection URL. collection_name character string specifying name collection. db_name character string specifying name database.","code":""},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/reference/mdb_collection_push.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Upload Data to MongoDB and Overwrite Existing Content — mdb_collection_push","text":"number data documents inserted collection (excluding order document).","code":""},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/reference/mdb_collection_push.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Upload Data to MongoDB and Overwrite Existing Content — mdb_collection_push","text":"","code":"if (FALSE) { # \\dontrun{ # Upload and overwrite data in a MongoDB collection result <- mdb_collection_push(   data = processed_legacy_landings,   connection_string = \"mongodb://localhost:27017\",   collection_name = \"my_collection\",   db_name = \"my_database\" ) } # }"},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/reference/merge_landings.html","id":null,"dir":"Reference","previous_headings":"","what":"Merge Legacy and Ongoing Landings Data — merge_landings","title":"Merge Legacy and Ongoing Landings Data — merge_landings","text":"function merges preprocessed legacy landings data ongoing landings data Google Cloud Storage. combines datasets, performs minimal transformations, uploads merged result Parquet file.","code":""},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/reference/merge_landings.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Merge Legacy and Ongoing Landings Data — merge_landings","text":"","code":"merge_landings(log_threshold = logger::DEBUG)"},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/reference/merge_landings.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Merge Legacy and Ongoing Landings Data — merge_landings","text":"log_threshold Logging threshold level (default: logger::DEBUG)","code":""},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/reference/merge_landings.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Merge Legacy and Ongoing Landings Data — merge_landings","text":"return value. Function processes data uploads result Parquet file Google Cloud Storage.","code":""},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/reference/merge_landings.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Merge Legacy and Ongoing Landings Data — merge_landings","text":"function performs following main operations: Downloads preprocessed legacy data Google Cloud Storage. Downloads preprocessed ongoing data Google Cloud Storage. Combines two datasets using dplyr::bind_rows(), adding 'version' column distinguish sources. Selects orders relevant columns final merged dataset. Uploads merged data Parquet file Google Cloud Storage.","code":""},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/reference/merge_landings.html","id":"note","dir":"Reference","previous_headings":"","what":"Note","title":"Merge Legacy and Ongoing Landings Data — merge_landings","text":"function requires configuration file Google Cloud Storage credentials file prefix settings.","code":""},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/reference/merge_landings.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Merge Legacy and Ongoing Landings Data — merge_landings","text":"","code":"if (FALSE) { # \\dontrun{ merge_landings() } # }"},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/reference/merge_prices.html","id":null,"dir":"Reference","previous_headings":"","what":"Merge Price Data — merge_prices","title":"Merge Price Data — merge_prices","text":"function combines processes legacy ongoing catch price data MongoDB collections, aggregating prices year uploading results back MongoDB.","code":""},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/reference/merge_prices.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Merge Price Data — merge_prices","text":"","code":"merge_prices(log_threshold = logger::DEBUG)"},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/reference/merge_prices.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Merge Price Data — merge_prices","text":"log_threshold Logging threshold level (default: logger::DEBUG)","code":""},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/reference/merge_prices.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Merge Price Data — merge_prices","text":"tibble containing processed combined price data","code":""},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/reference/merge_prices.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Merge Price Data — merge_prices","text":"function performs following main operations: Pulls legacy price data MongoDB summarizes yearly Pulls ongoing price data MongoDB summarizes yearly Combines legacy ongoing data Filters data 1990 Removes duplicate entries Uploads processed data back MongoDB","code":""},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/reference/merge_prices.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Merge Price Data — merge_prices","text":"","code":"if (FALSE) { # \\dontrun{ merge_prices() } # }"},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/reference/pipe.html","id":null,"dir":"Reference","previous_headings":"","what":"Pipe operator — %>%","title":"Pipe operator — %>%","text":"See magrittr::%>% details.","code":""},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/reference/pipe.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Pipe operator — %>%","text":"","code":"lhs %>% rhs"},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/reference/preprocess_landings_core.html","id":null,"dir":"Reference","previous_headings":"","what":"Core preprocessing logic for landings data — preprocess_landings_core","title":"Core preprocessing logic for landings data — preprocess_landings_core","text":"Core preprocessing logic landings data","code":""},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/reference/preprocess_landings_core.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Core preprocessing logic for landings data — preprocess_landings_core","text":"","code":"preprocess_landings_core(raw_dat, gear_mapping_func)"},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/reference/preprocess_landings_core.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Core preprocessing logic for landings data — preprocess_landings_core","text":"raw_dat Raw data cloud storage gear_mapping_func Function handles gear standardization","code":""},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/reference/preprocess_landings_core.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Core preprocessing logic for landings data — preprocess_landings_core","text":"Preprocessed landings data","code":""},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/reference/preprocess_landings_v1.html","id":null,"dir":"Reference","previous_headings":"","what":"Preprocess Landings Data (Version 1) — preprocess_landings_v1","title":"Preprocess Landings Data (Version 1) — preprocess_landings_v1","text":"function preprocesses raw landings data Google Cloud Storage. performs various data cleaning transformation operations, including column renaming, data pivoting, standardization catch names.","code":""},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/reference/preprocess_landings_v1.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Preprocess Landings Data (Version 1) — preprocess_landings_v1","text":"","code":"preprocess_landings_v1(log_threshold = logger::DEBUG)"},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/reference/preprocess_landings_v1.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Preprocess Landings Data (Version 1) — preprocess_landings_v1","text":"log_threshold Logging threshold level (default: logger::DEBUG)","code":""},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/reference/preprocess_landings_v1.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Preprocess Landings Data (Version 1) — preprocess_landings_v1","text":"return value. Function processes data uploads result Parquet file Google Cloud Storage.","code":""},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/reference/preprocess_landings_v1.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Preprocess Landings Data (Version 1) — preprocess_landings_v1","text":"function performs following main operations: Downloads raw data Google Cloud Storage Renames columns selects relevant fields Generates unique survey IDs Cleans standardizes text fields Pivots catch data wide long format Standardizes catch names separates size information Converts data types handles cases catch data Uploads processed data Parquet file Google Cloud Storage","code":""},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/reference/preprocess_landings_v1.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Preprocess Landings Data (Version 1) — preprocess_landings_v1","text":"","code":"if (FALSE) { # \\dontrun{ preprocessed_data <- preprocess_landings_v1() } # }"},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/reference/preprocess_landings_v2.html","id":null,"dir":"Reference","previous_headings":"","what":"Preprocess Landings Data (Version 2) — preprocess_landings_v2","title":"Preprocess Landings Data (Version 2) — preprocess_landings_v2","text":"function preprocesses raw landings data Google Cloud Storage. performs various data cleaning transformation operations, including column renaming, data pivoting, standardization catch names.","code":""},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/reference/preprocess_landings_v2.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Preprocess Landings Data (Version 2) — preprocess_landings_v2","text":"","code":"preprocess_landings_v2(log_threshold = logger::DEBUG)"},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/reference/preprocess_landings_v2.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Preprocess Landings Data (Version 2) — preprocess_landings_v2","text":"log_threshold Logging threshold level (default: logger::DEBUG)","code":""},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/reference/preprocess_landings_v2.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Preprocess Landings Data (Version 2) — preprocess_landings_v2","text":"return value. Function processes data uploads result Parquet file Google Cloud Storage.","code":""},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/reference/preprocess_landings_v2.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Preprocess Landings Data (Version 2) — preprocess_landings_v2","text":"function performs following main operations: Downloads raw data Google Cloud Storage Renames columns selects relevant fields Generates unique survey IDs Cleans standardizes text fields Pivots catch data wide long format Standardizes catch names separates size information Converts data types handles cases catch data Uploads processed data Parquet file Google Cloud Storage","code":""},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/reference/preprocess_landings_v2.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Preprocess Landings Data (Version 2) — preprocess_landings_v2","text":"","code":"if (FALSE) { # \\dontrun{ preprocessed_data <- preprocess_landings_v2() } # }"},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/reference/preprocess_legacy_landings.html","id":null,"dir":"Reference","previous_headings":"","what":"Preprocess Legacy Landings Data — preprocess_legacy_landings","title":"Preprocess Legacy Landings Data — preprocess_legacy_landings","text":"function imports, preprocesses, cleans legacy landings data MongoDB collection. performs various data cleaning transformation operations, including column renaming, removal unnecessary columns, generation unique identifiers, data type conversions. processed data uploaded back MongoDB.","code":""},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/reference/preprocess_legacy_landings.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Preprocess Legacy Landings Data — preprocess_legacy_landings","text":"","code":"preprocess_legacy_landings(log_threshold = logger::DEBUG)"},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/reference/preprocess_legacy_landings.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Preprocess Legacy Landings Data — preprocess_legacy_landings","text":"log_threshold Logging threshold level (default: logger::DEBUG)","code":""},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/reference/preprocess_legacy_landings.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Preprocess Legacy Landings Data — preprocess_legacy_landings","text":"function return value. Instead, processes data uploads result MongoDB collection pipeline database.","code":""},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/reference/preprocess_legacy_landings.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Preprocess Legacy Landings Data — preprocess_legacy_landings","text":"function performs following main operations: Pulls raw data raw data MongoDB collection. Removes several unnecessary columns. Renames columns clarity (e.g., 'site' 'landing_site'). Generates unique 'survey_id' 'catch_id' fields. Converts several string fields lowercase. Cleans catch names using separate function 'clean_catch_names'. Uploads processed data preprocessed MongoDB collection.","code":""},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/reference/preprocess_legacy_landings.html","id":"note","dir":"Reference","previous_headings":"","what":"Note","title":"Preprocess Legacy Landings Data — preprocess_legacy_landings","text":"function requires configuration file present readable 'read_config' function, provide MongoDB connection details.","code":""},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/reference/preprocess_legacy_landings.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Preprocess Legacy Landings Data — preprocess_legacy_landings","text":"","code":"if (FALSE) { # \\dontrun{ preprocess_legacy_landings() } # }"},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/reference/preprocess_pds_tracks.html","id":null,"dir":"Reference","previous_headings":"","what":"Preprocess Pelagic Data Systems (PDS) Track Data — preprocess_pds_tracks","title":"Preprocess Pelagic Data Systems (PDS) Track Data — preprocess_pds_tracks","text":"Downloads raw GPS tracks creates gridded summary fishing activity.","code":""},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/reference/preprocess_pds_tracks.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Preprocess Pelagic Data Systems (PDS) Track Data — preprocess_pds_tracks","text":"","code":"preprocess_pds_tracks(log_threshold = logger::DEBUG, grid_size = 500)"},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/reference/preprocess_pds_tracks.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Preprocess Pelagic Data Systems (PDS) Track Data — preprocess_pds_tracks","text":"log_threshold logging threshold use. Default logger::DEBUG. grid_size Numeric. Size grid cells meters (100, 250, 500, 1000).","code":""},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/reference/preprocess_pds_tracks.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Preprocess Pelagic Data Systems (PDS) Track Data — preprocess_pds_tracks","text":"None (invisible). Creates uploads preprocessed files.","code":""},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/reference/preprocess_price_core.html","id":null,"dir":"Reference","previous_headings":"","what":"Core preprocessing logic for price data — preprocess_price_core","title":"Core preprocessing logic for price data — preprocess_price_core","text":"Core preprocessing logic price data","code":""},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/reference/preprocess_price_core.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Core preprocessing logic for price data — preprocess_price_core","text":"","code":"preprocess_price_core(raw_dat, version)"},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/reference/preprocess_price_core.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Core preprocessing logic for price data — preprocess_price_core","text":"raw_dat Raw data cloud storage version Version identifier logging processing","code":""},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/reference/preprocess_price_core.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Core preprocessing logic for price data — preprocess_price_core","text":"Preprocessed price data","code":""},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/reference/preprocess_price_landings.html","id":null,"dir":"Reference","previous_headings":"","what":"Preprocess Price Data — preprocess_price_landings","title":"Preprocess Price Data — preprocess_price_landings","text":"function preprocesses raw price data Google Cloud Storage. performs various data cleaning transformation operations, including column renaming, data pivoting, standardization fish categories prices.","code":""},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/reference/preprocess_price_landings.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Preprocess Price Data — preprocess_price_landings","text":"","code":"preprocess_price_landings(log_threshold = logger::DEBUG)"},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/reference/preprocess_price_landings.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Preprocess Price Data — preprocess_price_landings","text":"log_threshold Logging threshold level (default: logger::DEBUG)","code":""},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/reference/preprocess_price_landings.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Preprocess Price Data — preprocess_price_landings","text":"return value. Function processes data uploads result Parquet file Google Cloud Storage.","code":""},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/reference/preprocess_price_landings.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Preprocess Price Data — preprocess_price_landings","text":"function performs following main operations: Downloads raw price data Google Cloud Storage Renames columns selects relevant fields (submission_id, landing_site, landing_date, price fields) Cleans standardizes text fields Pivots price data wide long format Standardizes fish category names separates size information Converts data types (datetime, character, numeric) Removes duplicate entries Uploads processed data Parquet file Google Cloud Storage","code":""},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/reference/preprocess_price_landings.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Preprocess Price Data — preprocess_price_landings","text":"","code":"if (FALSE) { # \\dontrun{ preprocess_price_landings() } # }"},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/reference/preprocess_track_data.html","id":null,"dir":"Reference","previous_headings":"","what":"Preprocess Track Data into Spatial Grid Summary — preprocess_track_data","title":"Preprocess Track Data into Spatial Grid Summary — preprocess_track_data","text":"function processes GPS track data spatial grid summary, calculating time spent metrics grid cell. grid size can specified analyze spatial patterns different scales.","code":""},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/reference/preprocess_track_data.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Preprocess Track Data into Spatial Grid Summary — preprocess_track_data","text":"","code":"preprocess_track_data(data, grid_size = 500)"},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/reference/preprocess_track_data.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Preprocess Track Data into Spatial Grid Summary — preprocess_track_data","text":"data data frame containing GPS track data columns: Trip: Unique trip identifier Time: Timestamp GPS point Lat: Latitude Lng: Longitude Speed (M/S): Speed meters per second Range (Meters): Range meters Heading: Heading degrees grid_size Numeric. Size grid cells meters. Must one : 100: ~100m grid cells 250: ~250m grid cells 500: ~500m grid cells (default) 1000: ~1km grid cells","code":""},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/reference/preprocess_track_data.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Preprocess Track Data into Spatial Grid Summary — preprocess_track_data","text":"tibble following columns: Trip: Trip identifier lat_grid: Latitude grid cell center lng_grid: Longitude grid cell center time_spent_mins: Total time spent grid cell minutes mean_speed: Average speed grid cell (M/S) mean_range: Average range grid cell (Meters) first_seen: First timestamp grid cell last_seen: Last timestamp grid cell n_points: Number GPS points grid cell","code":""},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/reference/preprocess_track_data.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Preprocess Track Data into Spatial Grid Summary — preprocess_track_data","text":"function creates grid rounding coordinates based specified grid size. Grid sizes approximate due conversion meters degrees, calculations based 1 degree ≈ 111km equator. Time spent calculated using time differences consecutive points.","code":""},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/reference/preprocess_track_data.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Preprocess Track Data into Spatial Grid Summary — preprocess_track_data","text":"","code":"if (FALSE) { # \\dontrun{ # Process tracks with 500m grid (default) result_500m <- preprocess_track_data(tracks_data)  # Use 100m grid for finer resolution result_100m <- preprocess_track_data(tracks_data, grid_size = 100)  # Use 1km grid for broader patterns result_1km <- preprocess_track_data(tracks_data, grid_size = 1000) } # }"},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/reference/process_single_track.html","id":null,"dir":"Reference","previous_headings":"","what":"Process Single PDS Track — process_single_track","title":"Process Single PDS Track — process_single_track","text":"Process Single PDS Track","code":""},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/reference/process_single_track.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Process Single PDS Track — process_single_track","text":"","code":"process_single_track(trip_id, pars)"},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/reference/process_single_track.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Process Single PDS Track — process_single_track","text":"trip_id Character. ID trip process. pars List. Configuration parameters.","code":""},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/reference/process_single_track.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Process Single PDS Track — process_single_track","text":"List processing status details.","code":""},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/reference/read_config.html","id":null,"dir":"Reference","previous_headings":"","what":"Read configuration file — read_config","title":"Read configuration file — read_config","text":"Reads configuration file config.yml adds logging lines. Wrapped convenience","code":""},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/reference/read_config.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Read configuration file — read_config","text":"","code":"read_config()"},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/reference/read_config.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Read configuration file — read_config","text":"environment parameters","code":""},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/reference/rename_child.html","id":null,"dir":"Reference","previous_headings":"","what":"Rename Child Elements in Nested Kobotoolbox Data — rename_child","title":"Rename Child Elements in Nested Kobotoolbox Data — rename_child","text":"internal function renames child elements nested Kobotoolbox data structures.","code":""},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/reference/rename_child.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Rename Child Elements in Nested Kobotoolbox Data — rename_child","text":"","code":"rename_child(x, i, p)"},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/reference/rename_child.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Rename Child Elements in Nested Kobotoolbox Data — rename_child","text":"x child element renamed. index name child element. p name parent element.","code":""},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/reference/rename_child.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Rename Child Elements in Nested Kobotoolbox Data — rename_child","text":"renamed list child elements.","code":""},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/reference/summarise_catch_price.html","id":null,"dir":"Reference","previous_headings":"","what":"Summarize Catch Price Data — summarise_catch_price","title":"Summarize Catch Price Data — summarise_catch_price","text":"function aggregates catch price data specified time unit, calculating median prices per kilogram fish category landing site.","code":""},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/reference/summarise_catch_price.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Summarize Catch Price Data — summarise_catch_price","text":"","code":"summarise_catch_price(data = NULL, unit = NULL)"},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/reference/summarise_catch_price.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Summarize Catch Price Data — summarise_catch_price","text":"data tibble containing catch price data columns: landing_date, landing_site, fish_category, ksh_kg unit Character string specifying time unit aggregation (e.g., \"year\", \"month\", \"week\"). Passed lubridate::floor_date()","code":""},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/reference/summarise_catch_price.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Summarize Catch Price Data — summarise_catch_price","text":"tibble containing summarized price data columns: date, landing_site, fish_category, size, median_ksh_kg","code":""},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/reference/summarise_catch_price.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Summarize Catch Price Data — summarise_catch_price","text":"function: Floors dates specified unit using lubridate Groups data date, landing site, fish category size Calculates median price per kilogram group","code":""},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/reference/summarise_catch_price.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Summarize Catch Price Data — summarise_catch_price","text":"","code":"if (FALSE) { # \\dontrun{ summarise_catch_price(data = price_data, unit = \"year\") summarise_catch_price(data = price_data, unit = \"month\") } # }"},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/reference/tidyeval.html","id":null,"dir":"Reference","previous_headings":"","what":"Tidy eval helpers — tidyeval","title":"Tidy eval helpers — tidyeval","text":"sym() creates symbol string syms() creates list symbols character vector. enquo() enquos() delay execution one several function arguments. enquo() returns single quoted expression, like blueprint delayed computation. enquos() returns list quoted expressions. expr() quotes new expression locally. mostly useful build new expressions around arguments captured enquo() enquos(): expr(mean(!!enquo(arg), na.rm = TRUE)). as_name() transforms quoted variable name string. Supplying something else quoted variable name error. unlike as_label() also returns single string supports kind R object input, including quoted function calls vectors. purpose summarise object single label. label often suitable default name. know quoted expression contains (instance expressions captured enquo() variable name, call function, unquoted constant), use as_label(). know quoted simple variable name, like enforce , use as_name(). learn tidy eval use tools, visit https://tidyeval.tidyverse.org Metaprogramming section Advanced R.","code":""},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/reference/update_airtable_record.html","id":null,"dir":"Reference","previous_headings":"","what":"Update Single Airtable Record — update_airtable_record","title":"Update Single Airtable Record — update_airtable_record","text":"Updates specific fields one record.","code":""},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/reference/update_airtable_record.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Update Single Airtable Record — update_airtable_record","text":"","code":"update_airtable_record(base_id, table_name, token, record_id, updates)"},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/reference/update_airtable_record.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Update Single Airtable Record — update_airtable_record","text":"base_id Character string. Airtable base ID. table_name Character string. Name table. token Character string. Airtable API token. record_id Character string. ID record update. updates Named list. Fields values update.","code":""},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/reference/update_airtable_record.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Update Single Airtable Record — update_airtable_record","text":"httr2 response object.","code":""},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/reference/upload_cloud_file.html","id":null,"dir":"Reference","previous_headings":"","what":"Upload File to Cloud Storage — upload_cloud_file","title":"Upload File to Cloud Storage — upload_cloud_file","text":"Uploads local file specified cloud storage bucket, supporting single multiple files.","code":""},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/reference/upload_cloud_file.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Upload File to Cloud Storage — upload_cloud_file","text":"","code":"upload_cloud_file(file, provider, options, name = file)"},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/reference/upload_cloud_file.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Upload File to Cloud Storage — upload_cloud_file","text":"file character vector specifying path(s) file(s) upload. provider character string specifying cloud provider (\"gcs\" \"aws\"). options named list provider-specific options including bucket authentication details. name (Optional) name assign file cloud. specified, local file name used.","code":""},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/reference/upload_cloud_file.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Upload File to Cloud Storage — upload_cloud_file","text":"list metadata objects uploaded files successful.","code":""},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/reference/upload_cloud_file.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Upload File to Cloud Storage — upload_cloud_file","text":"GCS, options list must include: bucket: name bucket files uploaded. service_account_key: authentication JSON contents, previously authenticated. function utilizes googleCloudStorageR::gcs_upload() file uploads GCS.","code":""},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/reference/upload_cloud_file.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Upload File to Cloud Storage — upload_cloud_file","text":"","code":"if (FALSE) { # \\dontrun{ authentication_details <- readLines(\"path/to/json_file.json\") upload_cloud_file(   \"path/to/local_file.csv\",   \"gcs\",   list(service_account_key = authentication_details, bucket = \"my-bucket\") ) } # }"},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/reference/upload_parquet_to_cloud.html","id":null,"dir":"Reference","previous_headings":"","what":"Upload Processed Data to Cloud Storage — upload_parquet_to_cloud","title":"Upload Processed Data to Cloud Storage — upload_parquet_to_cloud","text":"function handles process writing data parquet file uploading cloud storage.","code":""},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/reference/upload_parquet_to_cloud.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Upload Processed Data to Cloud Storage — upload_parquet_to_cloud","text":"","code":"upload_parquet_to_cloud(   data,   prefix,   provider,   options,   compression = \"lz4\",   compression_level = 12 )"},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/reference/upload_parquet_to_cloud.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Upload Processed Data to Cloud Storage — upload_parquet_to_cloud","text":"data data frame tibble upload prefix file prefix path cloud storage provider cloud storage provider key options Cloud storage provider options compression Compression algorithm use (default: \"lz4\") compression_level Compression level (default: 12)","code":""},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/reference/upload_parquet_to_cloud.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Upload Processed Data to Cloud Storage — upload_parquet_to_cloud","text":"Invisible NULL","code":""},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/reference/upload_parquet_to_cloud.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Upload Processed Data to Cloud Storage — upload_parquet_to_cloud","text":"","code":"if (FALSE) { # \\dontrun{ upload_parquet_to_cloud(   data = processed_data,   prefix = conf$ingestion$koboform$catch$legacy$preprocessed,   provider = conf$storage$google$key,   options = conf$storage$google$options ) } # }"},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/reference/validate_catch.html","id":null,"dir":"Reference","previous_headings":"","what":"Validate Individual Catch Data — validate_catch","title":"Validate Individual Catch Data — validate_catch","text":"Compares fish group catch (catch_kg) upper bounds flags values exceed bound. Values exceeding bounds set NA catch_kg column.","code":""},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/reference/validate_catch.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Validate Individual Catch Data — validate_catch","text":"","code":"validate_catch(data = NULL, k = NULL, flag_value = NULL)"},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/reference/validate_catch.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Validate Individual Catch Data — validate_catch","text":"data data frame containing columns: catch_id, gear, fish_category, catch_kg. k numeric value passed get_catch_bounds outlier detection. flag_value numeric value use flag catches exceeding upper bound. Default 4.","code":""},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/reference/validate_catch.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Validate Individual Catch Data — validate_catch","text":"data frame columns: submission_id, catch_id, catch_kg, alert_catch.","code":""},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/reference/validate_catch_iqr.html","id":null,"dir":"Reference","previous_headings":"","what":"Validate Individual Catch Data using IQR method — validate_catch_iqr","title":"Validate Individual Catch Data using IQR method — validate_catch_iqr","text":"Validate Individual Catch Data using IQR method","code":""},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/reference/validate_catch_iqr.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Validate Individual Catch Data using IQR method — validate_catch_iqr","text":"","code":"validate_catch_iqr(data = NULL, multiplier = 1.5, flag_value = NULL)"},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/reference/validate_catch_iqr.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Validate Individual Catch Data using IQR method — validate_catch_iqr","text":"data data frame containing required columns multiplier multiplier IQR range (default 1.5) flag_value numeric value use flag catches exceeding bounds","code":""},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/reference/validate_catch_iqr.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Validate Individual Catch Data using IQR method — validate_catch_iqr","text":"data frame validated catch data alert flags","code":""},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/reference/validate_dates.html","id":null,"dir":"Reference","previous_headings":"","what":"Validate Landing Dates — validate_dates","title":"Validate Landing Dates — validate_dates","text":"function checks validity landing_date provided dataset. landing_date 1990-01-01, alert (error label) number 1 triggered. landing_date set NA records.","code":""},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/reference/validate_dates.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Validate Landing Dates — validate_dates","text":"","code":"validate_dates(data = NULL, flag_value = NULL)"},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/reference/validate_dates.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Validate Landing Dates — validate_dates","text":"data data frame containing landing_date column. flag_value numeric value use flag catches exceeding upper bound.","code":""},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/reference/validate_dates.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Validate Landing Dates — validate_dates","text":"data frame two columns: landing_date alert_date. landing_date: original date valid, otherwise NA. alert_date: numeric value indicating alert (error label) number, 1 represents invalid date.","code":""},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/reference/validate_dates.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Validate Landing Dates — validate_dates","text":"","code":"if (FALSE) { # \\dontrun{ validate_dates(data, flag_value = 1) } # }"},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/reference/validate_fishers_catch.html","id":null,"dir":"Reference","previous_headings":"","what":"Validate Catch per Fisher — validate_fishers_catch","title":"Validate Catch per Fisher — validate_fishers_catch","text":"function validates relationship total catch number fishers. flags cases single fisher reports catch exceeding specified maximum. flagged, total catch value set NA.","code":""},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/reference/validate_fishers_catch.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Validate Catch per Fisher — validate_fishers_catch","text":"","code":"validate_fishers_catch(data = NULL, max_kg = NULL, flag_value = NULL)"},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/reference/validate_fishers_catch.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Validate Catch per Fisher — validate_fishers_catch","text":"data data frame containing columns: submission_id: Unique identifier submission no_of_fishers: Number fishers total_catch_kg: Total catch kilograms max_kg Numeric value specifying maximum catch (kg) allowed single fisher flag_value numeric value use flag catches exceeding maximum per fisher","code":""},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/reference/validate_fishers_catch.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Validate Catch per Fisher — validate_fishers_catch","text":"data frame columns: submission_id: original submission identifier total_catch_kg: original catch valid, otherwise NA alert_catch: Flag value catch per fisher exceeds maximum, otherwise NA","code":""},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/reference/validate_fishers_catch.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Validate Catch per Fisher — validate_fishers_catch","text":"","code":"if (FALSE) { # \\dontrun{ validate_fishers_catch(data, max_kg = 100, flag_value = 5) } # }"},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/reference/validate_landings.html","id":null,"dir":"Reference","previous_headings":"","what":"Validate Fisheries Data — validate_landings","title":"Validate Fisheries Data — validate_landings","text":"function imports validates preprocessed fisheries data Google Cloud Storage. conducts series validation checks ensure data integrity, including checks dates, fisher counts, boat numbers, catch weights. function compiles validated data corresponding alert flags, subsequently uploaded back Google Cloud Storage.","code":""},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/reference/validate_landings.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Validate Fisheries Data — validate_landings","text":"","code":"validate_landings()"},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/reference/validate_landings.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Validate Fisheries Data — validate_landings","text":"return value. Function processes data uploads validated results Parquet files Google Cloud Storage.","code":""},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/reference/validate_landings.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Validate Fisheries Data — validate_landings","text":"function performs following main operations: Downloads preprocessed landings data Google Cloud Storage. Validates data consistency accuracy, focusing : Date validation Number fishers Number boats Catch weight Generates validated dataset integrates results validation checks. Creates alert flags identify track data issues discovered validation. Merges validated data additional metadata. Uploads validated dataset alert flags Parquet files Google Cloud Storage.","code":""},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/reference/validate_landings.html","id":"note","dir":"Reference","previous_headings":"","what":"Note","title":"Validate Fisheries Data — validate_landings","text":"function requires configuration file Google Cloud Storage credentials parameters validation.","code":""},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/reference/validate_nboats.html","id":null,"dir":"Reference","previous_headings":"","what":"Validate Number of Boats — validate_nboats","title":"Validate Number of Boats — validate_nboats","text":"function validates n_boats column provided dataset. alert (error label) triggered number boats outlier, determined alert_outlier function specified parameters. alert number stored alert_n_boats. alert triggered, n_boats value set NA.","code":""},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/reference/validate_nboats.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Validate Number of Boats — validate_nboats","text":"","code":"validate_nboats(data = NULL, k = NULL, flag_value = NULL)"},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/reference/validate_nboats.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Validate Number of Boats — validate_nboats","text":"data data frame containing n_boats column. k numeric value used LocScaleB function outlier detection. flag_value numeric value use flag catches exceeding upper bound.","code":""},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/reference/validate_nboats.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Validate Number of Boats — validate_nboats","text":"data frame two columns: n_boats alert_n_boats. n_boats: original number boats valid, otherwise NA. alert_n_boats: numeric value indicating alert (error label) number.","code":""},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/reference/validate_nboats.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Validate Number of Boats — validate_nboats","text":"","code":"if (FALSE) { # \\dontrun{ validate_nboats(data, k = 2, flag_value = 3) } # }"},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/reference/validate_nboats_iqr.html","id":null,"dir":"Reference","previous_headings":"","what":"Validate Number of Boats using IQR method — validate_nboats_iqr","title":"Validate Number of Boats using IQR method — validate_nboats_iqr","text":"Validate Number Boats using IQR method","code":""},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/reference/validate_nboats_iqr.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Validate Number of Boats using IQR method — validate_nboats_iqr","text":"","code":"validate_nboats_iqr(data = NULL, multiplier = 1.5, flag_value = NULL)"},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/reference/validate_nboats_iqr.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Validate Number of Boats using IQR method — validate_nboats_iqr","text":"data data frame containing n_boats column multiplier multiplier IQR range (default 1.5) flag_value numeric value use flag values outside bounds","code":""},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/reference/validate_nboats_iqr.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Validate Number of Boats using IQR method — validate_nboats_iqr","text":"data frame validated n_boats alert flags","code":""},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/reference/validate_nboats_iqr.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Validate Number of Boats using IQR method — validate_nboats_iqr","text":"","code":"if (FALSE) { # \\dontrun{ validate_nboats_iqr(data, multiplier = 1.5, flag_value = 8) } # }"},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/reference/validate_nfishers.html","id":null,"dir":"Reference","previous_headings":"","what":"Validate Number of Fishers — validate_nfishers","title":"Validate Number of Fishers — validate_nfishers","text":"function validates no_of_fishers column provided dataset. alert (error label) triggered number fishers outlier, determined alert_outlier function specified parameters. alert number stored alert_n_fishers. alert triggered, no_of_fishers value set NA.","code":""},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/reference/validate_nfishers.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Validate Number of Fishers — validate_nfishers","text":"","code":"validate_nfishers(data = NULL, k = NULL, flag_value = NULL)"},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/reference/validate_nfishers.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Validate Number of Fishers — validate_nfishers","text":"data data frame containing no_of_fishers column. k numeric value used LocScaleB function outlier detection. flag_value numeric value use flag catches exceeding upper bound.","code":""},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/reference/validate_nfishers.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Validate Number of Fishers — validate_nfishers","text":"data frame two columns: no_of_fishers alert_n_fishers. no_of_fishers: original number fishers valid, otherwise NA. alert_n_fishers: numeric value indicating alert (error label) number.","code":""},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/reference/validate_nfishers.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Validate Number of Fishers — validate_nfishers","text":"","code":"if (FALSE) { # \\dontrun{ validate_nfishers(data, k = 3, flag_value = 2) } # }"},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/reference/validate_nfishers_iqr.html","id":null,"dir":"Reference","previous_headings":"","what":"Validate Number of Fishers using IQR method — validate_nfishers_iqr","title":"Validate Number of Fishers using IQR method — validate_nfishers_iqr","text":"Validate Number Fishers using IQR method","code":""},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/reference/validate_nfishers_iqr.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Validate Number of Fishers using IQR method — validate_nfishers_iqr","text":"","code":"validate_nfishers_iqr(data = NULL, multiplier = 1.5, flag_value = NULL)"},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/reference/validate_nfishers_iqr.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Validate Number of Fishers using IQR method — validate_nfishers_iqr","text":"data data frame containing no_of_fishers column multiplier multiplier IQR range (default 1.5) flag_value numeric value use flag values outside bounds","code":""},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/reference/validate_nfishers_iqr.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Validate Number of Fishers using IQR method — validate_nfishers_iqr","text":"data frame validated no_of_fishers alert flags","code":""},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/reference/validate_nfishers_iqr.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Validate Number of Fishers using IQR method — validate_nfishers_iqr","text":"","code":"if (FALSE) { # \\dontrun{ validate_nfishers_iqr(data, multiplier = 1.5, flag_value = 7) } # }"},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/reference/validate_total_catch.html","id":null,"dir":"Reference","previous_headings":"","what":"Validate Total Catch Data — validate_total_catch","title":"Validate Total Catch Data — validate_total_catch","text":"Compares total catch (total_catch_kg) upper bounds flags values exceed bound. Values exceeding bounds set NA total_catch_kg column. Bounds calculated based landing site gear type combinations.","code":""},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/reference/validate_total_catch.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Validate Total Catch Data — validate_total_catch","text":"","code":"validate_total_catch(data = NULL, k = NULL, flag_value = NULL)"},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/reference/validate_total_catch.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Validate Total Catch Data — validate_total_catch","text":"data data frame containing columns: submission_id, landing_site, gear, total_catch_kg. k numeric value passed get_total_catch_bounds outlier detection. flag_value numeric value use flag catches exceeding upper bound. Default 4.","code":""},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/reference/validate_total_catch.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Validate Total Catch Data — validate_total_catch","text":"data frame columns: submission_id, total_catch_kg, alert_catch.","code":""},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/reference/validate_total_catch_iqr.html","id":null,"dir":"Reference","previous_headings":"","what":"Validate Total Catch Data using IQR method — validate_total_catch_iqr","title":"Validate Total Catch Data using IQR method — validate_total_catch_iqr","text":"Validate Total Catch Data using IQR method","code":""},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/reference/validate_total_catch_iqr.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Validate Total Catch Data using IQR method — validate_total_catch_iqr","text":"","code":"validate_total_catch_iqr(data = NULL, multiplier = 1.5, flag_value = NULL)"},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/reference/validate_total_catch_iqr.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Validate Total Catch Data using IQR method — validate_total_catch_iqr","text":"data data frame containing required columns multiplier multiplier IQR range (default 1.5) flag_value numeric value use flag catches exceeding bounds","code":""},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/reference/validate_total_catch_iqr.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Validate Total Catch Data using IQR method — validate_total_catch_iqr","text":"data frame validated total catch data alert flags","code":""},{"path":[]},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/news/index.html","id":"configuration-4-3-0","dir":"Changelog","previous_headings":"","what":"Configuration","title":"peskas.kenya.data.pipeline 4.3.0","text":"Migrated authentication file-based (auth/) environment variable approach using .env files Improved security deployment flexibility using environment variables credentials Added .env.example reference configuration","code":""},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/news/index.html","id":"fixes-4-3-0","dir":"Changelog","previous_headings":"","what":"Fixes","title":"peskas.kenya.data.pipeline 4.3.0","text":"Fixed function naming conflict: renamed get_fishery_metrics() preprocessing.R get_fishery_metrics_long() avoid duplicate function definitions Corrected parameter names helper functions (get_fishery_metrics(), get_individual_metrics(), get_individual_gear_metrics()) match documentation Resolved “unused arguments” error export_summaries() function calls Updated function documentation align actual parameter names","code":""},{"path":[]},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/news/index.html","id":"new-features-4-2-0","dir":"Changelog","previous_headings":"","what":"New features","title":"peskas.kenya.data.pipeline 4.2.0","text":"Added process_fishing_tracks() comprehensive GPS track processing fishing activity classification Implemented prepare_gps_data() convert raw tracking data GPSMonitoring format Added classify_fishing_activity() using speed thresholds spatial clustering identify fishing vs transit activities Created process_trajectories_with_speed() calculating vessel speeds GPS positions Implemented calculate_fishing_summaries() generate effort metrics trip spatial grid Added visualize_fishing_track() mapping fishing activities Created create_exclusion_zones() create_extent_polygon() spatial analysis Implemented bidirectional Airtable API integration airtable_to_df() reading records Added df_to_airtable() bulk create operations Created update_airtable_record() updating individual records Implemented bulk_update_airtable() batch update operations Added get_writable_fields() retrieve field schemas validate writable fields Extended get_fishery_metrics() normalized long format output maximum interoperability Added metrics CPUE RPUE gear type Implemented species composition analysis top 2 species ranking Created fully normalized dataset structure flexible aggregation filtering Added comprehensive GPS tracking visualization report (inst/reports/pds/trackers.qmd) Integrated interactive maps, time series, network analysis visualizations Implemented spatial heatmaps trip trajectory analysis","code":""},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/news/index.html","id":"enhancements-4-2-0","dir":"Changelog","previous_headings":"","what":"Enhancements","title":"peskas.kenya.data.pipeline 4.2.0","text":"Fixed R CMD check warnings replacing library()/require() calls :: notation requireNamespace() Added missing global variable bindings track fishery metrics variables Updated NAMESPACE new imports: dplyr::n, dplyr::arrange, dplyr::row_number, stats::lag, stats::time, rlang::sym Added GPSMonitoring ggplot2 Suggests dependencies Excluded .claude directory .parquet files package build via .Rbuildignore Reduced R CMD check 3 warnings/6 notes 2 warnings/2 notes Updated configuration (inst/config.yml) KEFS survey data integration Enhanced inst/kefs_integration.R improved data processing workflows","code":""},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/news/index.html","id":"documentation-4-2-0","dir":"Changelog","previous_headings":"","what":"Documentation","title":"peskas.kenya.data.pipeline 4.2.0","text":"process_fishing_tracks.Rd, prepare_gps_data.Rd, classify_fishing_activity.Rd process_trajectories_with_speed.Rd, calculate_fishing_summaries.Rd create_exclusion_zones.Rd, create_extent_polygon.Rd, visualize_fishing_track.Rd airtable_to_df.Rd, df_to_airtable.Rd, update_airtable_record.Rd bulk_update_airtable.Rd, get_writable_fields.Rd Updated get_fishery_metrics.Rd document dual function signatures different use cases Enhanced documentation comprehensive examples parameter descriptions","code":""},{"path":[]},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/news/index.html","id":"enhancements-4-1-0","dir":"Changelog","previous_headings":"","what":"Enhancements","title":"peskas.kenya.data.pipeline 4.1.0","text":"Enhanced mdb_collection_pull() properly handle MongoDB ObjectId fields explicitly including _id queries maintaining correct column ordering Updated column reordering logic prioritize _id field placement retrieved datasets Improved data consistency integrity pulling data MongoDB collections Standardized function parameter formatting across storage functions improved readability Enhanced code consistency upload_parquet_to_cloud(), cloud_object_name(), related functions Updated get_metadata() function formatting follow consistent style guidelines Added comprehensive user management system MongoDB dashboard treatment group support Implemented secure password generation user creation functionality Added support BMU-based user access control role assignment","code":""},{"path":[]},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/news/index.html","id":"new-features-3-2-1","dir":"Changelog","previous_headings":"","what":"New features","title":"peskas.kenya.data.pipeline 3.2.1","text":"Added get_individual_gear_metrics() calculates gear key fishery performance metrics individual fisher level validated catch data, including CPUE (Catch Per Unit Effort), RPUE (Revenue Per Unit Effort), price per kg, trip costs, profit margins fisher. Integrated fish composition distribution fishers level","code":""},{"path":[]},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/news/index.html","id":"new-features-3-2-0","dir":"Changelog","previous_headings":"","what":"New features","title":"peskas.kenya.data.pipeline 3.2.0","text":"Added get_individual_metrics() calculates key fishery performance metrics individual fisher level validated catch data, including CPUE (Catch Per Unit Effort), RPUE (Revenue Per Unit Effort), price per kg, trip costs, profit margins fisher.","code":""},{"path":[]},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/news/index.html","id":"new-features-3-1-0","dir":"Changelog","previous_headings":"","what":"New features","title":"peskas.kenya.data.pipeline 3.1.0","text":"Added get_individual_data() extract fisher IDs trip costs raw survey data, enabling granular analysis fishing effort expenses. Integrated individual fisher trip cost data version 2 preprocessing pipeline (preprocess_landings_v2()), new columns fisher_id trip_cost preprocessed outputs.","code":""},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/news/index.html","id":"enhancements-3-1-0","dir":"Changelog","previous_headings":"","what":"Enhancements","title":"peskas.kenya.data.pipeline 3.1.0","text":"Updated merge_landings() support merging legacy, v1, v2 preprocessed landings, including new fields individual fisher trip cost data. Improved column selection ordering merged datasets consistency downstream compatibility. Enhanced export_summaries() include new individual-level statistics, mean trip expenses per fisher, aggregated BMU month (data collected June 25, 2025). Improved monthly distribution summaries reflect expanded data model.","code":""},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/news/index.html","id":"configuration-3-1-0","dir":"Changelog","previous_headings":"","what":"Configuration","title":"peskas.kenya.data.pipeline 3.1.0","text":"Updated config.yml support new data paths fields required individual fisher trip cost data v2 surveys.","code":""},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/news/index.html","id":"documentation-3-1-0","dir":"Changelog","previous_headings":"","what":"Documentation","title":"peskas.kenya.data.pipeline 3.1.0","text":"Added new man page get_individual_data(). Updated documentation affected functions reflect new parameters outputs.","code":""},{"path":[]},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/news/index.html","id":"new-features-3-0-0","dir":"Changelog","previous_headings":"","what":"New features","title":"peskas.kenya.data.pipeline 3.0.0","text":"Added version-specific ingestion functions catch price surveys Implemented ingest_catch_survey_version() ingest_price_survey_version() versioned data handling Created versioned preprocessing functions (preprocess_landings_v1() preprocess_landings_v2()) Added core preprocessing functions modular data transformation","code":""},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/news/index.html","id":"enhancements-3-0-0","dir":"Changelog","previous_headings":"","what":"Enhancements","title":"peskas.kenya.data.pipeline 3.0.0","text":"Refactored ingestion logic handle multiple survey versions Enhanced preprocessing workflow version-specific transformations Updated configuration structure support versioned data paths Improved documentation versioned functions","code":""},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/news/index.html","id":"documentation-3-0-0","dir":"Changelog","previous_headings":"","what":"Documentation","title":"peskas.kenya.data.pipeline 3.0.0","text":"Added comprehensive documentation new versioned functions Updated existing function documentation reflect versioning changes Added new man pages version-specific functions Enhanced function descriptions examples","code":""},{"path":[]},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/news/index.html","id":"new-features-2-1-0","dir":"Changelog","previous_headings":"","what":"New features","title":"peskas.kenya.data.pipeline 2.1.0","text":"Added create_geos() function generate GeoJSON files regional-level fishery metrics Implemented spatial analysis assign BMUs nearest coastal regions Exported time series aggregated metrics (CPUE, CPUA, effort, RPUE, RPUA) regional level Added regional polygon geometries spatial visualization","code":""},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/news/index.html","id":"enhancements-2-1-0","dir":"Changelog","previous_headings":"","what":"Enhancements","title":"peskas.kenya.data.pipeline 2.1.0","text":"Revised fishery metrics calculation accurate temporal representation Implemented daily-average approach instead aggregate--divide method Maintained per-day units (kg/fisher/day, KES/km²/day) consistent scientific standards Fixed CPUE daily metrics calculating daily level first Filtered export data include records 2023 onwards","code":""},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/news/index.html","id":"fixes-2-1-0","dir":"Changelog","previous_headings":"","what":"Fixes","title":"peskas.kenya.data.pipeline 2.1.0","text":"Corrected handling missing information NA values instead zeros Fixed dependency logic pipeline Improved price table prefix path handling Updated configuration paths downloading preprocessed price legacy catch data","code":""},{"path":[]},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/news/index.html","id":"new-features-2-0-0","dir":"Changelog","previous_headings":"","what":"New features","title":"peskas.kenya.data.pipeline 2.0.0","text":"alert_outlier_iqr() IQR-based outlier detection check_outliers_iqr() basic outlier checking validate_nfishers_iqr() validate_nboats_iqr() validating fisher boat counts validate_catch_iqr() validate_total_catch_iqr() catch validation get_catch_bounds_iqr() get_total_catch_bounds_iqr() calculating bounds Modified validate_landings() use IQR validation default","code":""},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/news/index.html","id":"enhancements-2-0-0","dir":"Changelog","previous_headings":"","what":"Enhancements","title":"peskas.kenya.data.pipeline 2.0.0","text":"Added comprehensive input validation IQR functions Added consistent NULL/empty data checks Improved NA value handling using dplyr::if_else Added warnings cases bounds calculated","code":""},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/news/index.html","id":"documentation-2-0-0","dir":"Changelog","previous_headings":"","what":"Documentation","title":"peskas.kenya.data.pipeline 2.0.0","text":"Added full documentation new IQR functions examples Updated NAMESPACE export new IQR functions Added proper Roxygen documentation new parameters","code":""},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/news/index.html","id":"other-changes-2-0-0","dir":"Changelog","previous_headings":"","what":"Other Changes","title":"peskas.kenya.data.pipeline 2.0.0","text":"Switched default validation method MAD IQR main pipeline Maintained backward compatibility existing MAD functions","code":""},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/news/index.html","id":"new-features-2-0-0-1","dir":"Changelog","previous_headings":"","what":"New features","title":"peskas.kenya.data.pipeline 2.0.0","text":"Replaced MongoDB storage Google Cloud Storage (GCS) using Parquet files Authentication connection management Upload download capabilities Versioned file handling Integrated Apache Arrow efficient Parquet file processing Updated configuration support GCS file prefixes paths","code":""},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/news/index.html","id":"enhancements-2-0-0-1","dir":"Changelog","previous_headings":"","what":"Enhancements","title":"peskas.kenya.data.pipeline 2.0.0","text":"Refactored storage operations use cloud-native approaches Improved data access performance Parquet file format Added support versioned file management","code":""},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/news/index.html","id":"dependencies-2-0-0","dir":"Changelog","previous_headings":"","what":"Dependencies","title":"peskas.kenya.data.pipeline 2.0.0","text":"Added cloud storage related libraries Added Arrow Parquet file handling","code":""},{"path":[]},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/news/index.html","id":"new-features-1-0-0","dir":"Changelog","previous_headings":"","what":"New features","title":"peskas.kenya.data.pipeline 1.0.0","text":"Added validate_fishers_catch() flag cases single fisher reports excessively high catch. Introduced impute_price() fill missing fish prices using median values across landing sites sizes. Integrated price validation validate_landings() ensure missing zero prices flagged. rpue (Revenue Per Unit Effort) aggregated price per fisher rpua (Revenue Per Unit Area) aggregated price per square km Updated GitHub Actions workflow (data-pipeline.yaml) include job processing merging price data. Added price-related MongoDB collections (raw_price, preprocessed_price, price_table) config.yml.","code":""},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/news/index.html","id":"enhancements-1-0-0","dir":"Changelog","previous_headings":"","what":"Enhancements","title":"peskas.kenya.data.pipeline 1.0.0","text":"validate_total_catch() validate_catch() now allow customizable flag_value parameters. Enhanced merge_prices() fixes site name inconsistencies (e.g., “Rigati” → “Rigata”). preprocess_legacy_landings() now ensures valid dates correcting invalid timestamps. export_summaries() now includes revenue metrics maintaining prior CPUE CPUA calculations. Updated .qmd reports reflect new price fields summary tables.","code":""},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/news/index.html","id":"fixes-1-0-0","dir":"Changelog","previous_headings":"","what":"Fixes","title":"peskas.kenya.data.pipeline 1.0.0","text":"Fixed potential issues merge_prices() lead duplicate price records. Ensured impute_price() correctly applies median imputation across relevant data points. Updated .Rd documentation modified functions reflect new parameters changes.","code":""},{"path":[]},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/news/index.html","id":"new-features-0-9-0","dir":"Changelog","previous_headings":"","what":"New features","title":"peskas.kenya.data.pipeline 0.9.0","text":"Introduced ingest_landings_price() download price surveys KoboToolbox. Implemented preprocess_price_landings() clean standardize fish price data. Added merge_prices() combine aggregate legacy ongoing price data. Created summarise_catch_price() compute median price per kilogram landing site fish category. Added new GitHub Actions job merge-price-data automate processing merging price data. Introduced new collections config.yml handling raw, preprocessed, aggregated price data (raw_price, preprocessed_price, price_table).","code":""},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/news/index.html","id":"enhancements-0-9-0","dir":"Changelog","previous_headings":"","what":"Enhancements","title":"peskas.kenya.data.pipeline 0.9.0","text":"Improved handling validation alert flags introducing dynamic flag_value parameters. Adjusted anomaly detection validate_landings() use descriptive alert codes. Updated .github/workflows/data-pipeline.yaml include price survey ingestion processing. Extended config.yml support price data processing local production environments. Exported new functions (ingest_landings_price, merge_prices, preprocess_price_landings, summarise_catch_price) make available package use.","code":""},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/news/index.html","id":"fixes-0-9-0","dir":"Changelog","previous_headings":"","what":"Fixes","title":"peskas.kenya.data.pipeline 0.9.0","text":"Updated .Rd documentation ingest_landings() clarify processes catch surveys (WCS surveys). Added descriptions new parameters (flag_value) validation function documentation. Resolved inconsistencies price data processing ensuring column renaming format standardization. Fixed logical errors merging price data prevent duplicate records. Improved MongoDB push operations correctly handle newly introduced price data collections.","code":""},{"path":[]},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/news/index.html","id":"new-features-0-8-0","dir":"Changelog","previous_headings":"","what":"New features","title":"peskas.kenya.data.pipeline 0.8.0","text":"Introduced flag_value parameter validation functions (validate_dates, validate_nfishers, validate_nboats, validate_catch) customizable alert thresholds. Added logical_check anomalous_submissions workflows improve anomaly detection validate_landings. Enhanced data upload pipeline purrr::walk2 streamlined MongoDB uploads validated flagged data.","code":""},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/news/index.html","id":"enhancements-0-8-0","dir":"Changelog","previous_headings":"","what":"Enhancements","title":"peskas.kenya.data.pipeline 0.8.0","text":"Updated export_summaries calculate mean_trip_catch using median improved robustness skewed data. Improved documentation validation functions include detailed descriptions examples new flag_value parameter. Refined logic validate_landings clearer alert flagging better handling edge cases. Added validation_flags collection configuration file (config.yml) centralized management flagging outputs. Updated anomaly alert flags use descriptive string values instead numeric codes better interpretability.","code":""},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/news/index.html","id":"fixes-0-8-0","dir":"Changelog","previous_headings":"","what":"Fixes","title":"peskas.kenya.data.pipeline 0.8.0","text":"Fixed inconsistencies handling alert flags data validation. Resolved potential mismatches merged datasets improving join logic validate_landings. Corrected documentation typos improved consistency across .Rd files.","code":""},{"path":[]},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/news/index.html","id":"new-features-0-7-0","dir":"Changelog","previous_headings":"","what":"New features","title":"peskas.kenya.data.pipeline 0.7.0","text":"Introduced get_total_catch_bounds calculating upper bounds total catch data grouped landing site gear type. Added validate_total_catch function validating total catch data improved outlier handling. Updated validate_landings identify filter inconsistent submissions (e.g., mismatched fishers boats). Enhanced validate_catch set outlier values catch_kg NA flag review.","code":""},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/news/index.html","id":"enhancements-0-7-0","dir":"Changelog","previous_headings":"","what":"Enhancements","title":"peskas.kenya.data.pipeline 0.7.0","text":"Improved get_catch_bounds exclude invalid categories clarified grouping logic gear fish categories. Updated documentation across validation catch bounds functions include better descriptions, keywords, return values. Refined data preprocessing preprocess_landings ensure unique rows, handle NA values, streamline workflow. Adjusted configuration parameters config.yml enhance validation sensitivity (e.g., increasing k_catch outlier detection). Updated export_summaries improve monthly summaries recalculating effort fishers per km² per day refining CPUE CPUA metrics include temporal normalization. Simplified mean trip catch calculations consistency across datasets.","code":""},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/news/index.html","id":"fixes-0-7-0","dir":"Changelog","previous_headings":"","what":"Fixes","title":"peskas.kenya.data.pipeline 0.7.0","text":"Corrected plot code formatting data_report.qmd CPUE, effort, CPUA visualizations. Fixed documentation typos inconsistencies .Rd files validation functions.","code":""},{"path":[]},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/news/index.html","id":"new-features-0-6-0","dir":"Changelog","previous_headings":"","what":"New features","title":"peskas.kenya.data.pipeline 0.6.0","text":"Add merge_landings function combining data sources Add validation catch function improve data quality checks Include form consent total catch fields merged data Calculate total catch legacy landings","code":""},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/news/index.html","id":"enhancements-0-6-0","dir":"Changelog","previous_headings":"","what":"Enhancements","title":"peskas.kenya.data.pipeline 0.6.0","text":"Homogenise gear fish groups names Update validation code structure Fix mismatch fish groups sum total catch Fix landing sites names Improve validation catch function Update export data based validation Fix legacy submission IDs","code":""},{"path":[]},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/news/index.html","id":"new-features-0-5-1","dir":"Changelog","previous_headings":"","what":"New features","title":"peskas.kenya.data.pipeline 0.5.1","text":"Add functions extract effort CPUE validated legacy data Implement MongoDB pushing functionality Add storage metadata functions","code":""},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/news/index.html","id":"enhancements-0-5-1","dir":"Changelog","previous_headings":"","what":"Enhancements","title":"peskas.kenya.data.pipeline 0.5.1","text":"Index functions topic package functional position Update package documentation website Improve MongoDB storage calls Add configuration file MongoDB database collection references","code":""},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/news/index.html","id":"fixes-0-5-1","dir":"Changelog","previous_headings":"","what":"Fixes","title":"peskas.kenya.data.pipeline 0.5.1","text":"Drop stale files Update storage calls better efficiency Fix various typos syntax errors","code":""},{"path":[]},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/news/index.html","id":"enhancements-0-5-0","dir":"Changelog","previous_headings":"","what":"Enhancements","title":"peskas.kenya.data.pipeline 0.5.0","text":"Improve mongoDB storage functions adding indexes improve query performance. fix column order data preserved.","code":""},{"path":[]},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/news/index.html","id":"new-features-0-4-0","dir":"Changelog","previous_headings":"","what":"New features","title":"peskas.kenya.data.pipeline 0.4.0","text":"Now ingest_surveys() uses Kobotoolbox API directly download surveys instead rely R package","code":""},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/news/index.html","id":"fixes-0-4-0","dir":"Changelog","previous_headings":"","what":"Fixes","title":"peskas.kenya.data.pipeline 0.4.0","text":"Fix ingestion functions limited download max 30,000 submissions. approach now uses pagination retrieve large datasets, limit 30,000 records per request","code":""},{"path":[]},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/news/index.html","id":"new-features-0-3-0","dir":"Changelog","previous_headings":"","what":"New features","title":"peskas.kenya.data.pipeline 0.3.0","text":"Add ingestion function ingest ongoing data ingest_surveys() Add preprocessing function preprocess ongoing data preprocess_landings()","code":""},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/news/index.html","id":"peskaskenyadatapipeline-020","dir":"Changelog","previous_headings":"","what":"peskas.kenya.data.pipeline 0.2.0","title":"peskas.kenya.data.pipeline 0.2.0","text":"Add package website documentation functions Integrate mongodb storage package functions Optimize configuration file","code":""},{"path":"https://worldfishcenter.github.io/peskas.kenya.data.pipeline/news/index.html","id":"peskaskenyadatapipeline-010","dir":"Changelog","previous_headings":"","what":"peskas.kenya.data.pipeline 0.1.0","title":"peskas.kenya.data.pipeline 0.1.0","text":"Initial CRAN submission.","code":""}]
